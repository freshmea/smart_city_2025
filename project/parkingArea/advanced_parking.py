"""
YOLO-OBB ê¸°ë°˜ ì •ë°€ ì£¼ì°¨ì¥ ë¶„ì„ ì‹œìŠ¤í…œ
ë” ì •í™•í•œ ì£¼ì°¨ ì˜ì—­ ê°ì§€ë¥¼ ìœ„í•œ ê³ ê¸‰ ì»´í“¨í„° ë¹„ì „ ê¸°ë²• ì ìš©
"""

import json
import os
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
from ultralytics import YOLO


class ParkingSpotStatus(Enum):
    """ì£¼ì°¨ ìƒíƒœ ì—´ê±°í˜•"""
    EMPTY = "empty"
    OCCUPIED = "occupied"
    UNKNOWN = "unknown"

@dataclass
class ParkingSpot:
    """ì£¼ì°¨ êµ¬ì—­ ì •ë³´"""
    id: int
    bbox: Tuple[int, int, int, int]  # (x1, y1, x2, y2)
    center: Tuple[int, int]
    area: float
    status: ParkingSpotStatus
    confidence: float
    corners: Optional[List[Tuple[int, int]]] = None  # OBB corners

class AdvancedParkingDetector:
    """YOLO-OBB ê¸°ë°˜ ê³ ê¸‰ ì£¼ì°¨ì¥ ê°ì§€ ì‹œìŠ¤í…œ"""

    def __init__(self, yolo_obb_path: str = None):
        """
        ì´ˆê¸°í™”
        Args:
            yolo_obb_path: YOLO-OBB ëª¨ë¸ ê²½ë¡œ
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")

        # YOLO-OBB ëª¨ë¸ ë¡œë“œ
        if yolo_obb_path and os.path.exists(yolo_obb_path):
            self.yolo_model = YOLO(yolo_obb_path)
            print(f"âœ… YOLO-OBB ëª¨ë¸ ë¡œë“œ: {yolo_obb_path}")
        else:
            # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            try:
                self.yolo_model = YOLO('yolov8n-obb.pt')
                print("âœ… ê¸°ë³¸ YOLO-OBB ëª¨ë¸ ë¡œë“œ")
            except:
                print("âš ï¸ YOLO-OBB ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ì „í†µì  ë°©ë²• ì‚¬ìš©")
                self.yolo_model = None

        self.parking_spots = []
        self.parking_template = None

    def preprocess_image_advanced(self, image: np.ndarray) -> Dict[str, np.ndarray]:
        """ê³ ê¸‰ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        results = {}

        # ì›ë³¸
        results['original'] = image.copy()

        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        results['grayscale'] = gray

        # ì ì‘í˜• íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)
        results['enhanced'] = enhanced

        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
        blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)
        results['blurred'] = blurred

        # ëª¨í´ë¡œì§€ ì—°ì‚°
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        morph = cv2.morphologyEx(blurred, cv2.MORPH_CLOSE, kernel)
        results['morphology'] = morph

        # ì ì‘í˜• ì„ê³„ê°’
        adaptive_thresh = cv2.adaptiveThreshold(
            morph, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY, 11, 2
        )
        results['adaptive_threshold'] = adaptive_thresh

        # Canny ì—£ì§€ (ì—¬ëŸ¬ ë§¤ê°œë³€ìˆ˜)
        canny1 = cv2.Canny(morph, 50, 150)
        canny2 = cv2.Canny(morph, 100, 200)
        canny_combined = cv2.bitwise_or(canny1, canny2)
        results['canny'] = canny_combined

        return results

    def detect_with_yolo_obb(self, image: np.ndarray) -> List[ParkingSpot]:
        """YOLO-OBBë¥¼ ì‚¬ìš©í•œ ì£¼ì°¨ ì˜ì—­ ê°ì§€"""
        if self.yolo_model is None:
            return []

        try:
            # YOLO ì¶”ë¡  ì‹¤í–‰
            results = self.yolo_model(image, verbose=False)
            parking_spots = []

            for i, result in enumerate(results):
                if hasattr(result, 'obb') and result.obb is not None:
                    # OBB (Oriented Bounding Box) ê²°ê³¼ ì²˜ë¦¬
                    for j, (obb, conf, cls) in enumerate(zip(result.obb.xyxyxyxy, result.obb.conf, result.obb.cls)):
                        if conf > 0.3:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                            # OBB ì¢Œí‘œë¥¼ ì¼ë°˜ bboxë¡œ ë³€í™˜
                            corners = obb.cpu().numpy().reshape(-1, 2)
                            x_coords = corners[:, 0]
                            y_coords = corners[:, 1]

                            x1, y1 = int(np.min(x_coords)), int(np.min(y_coords))
                            x2, y2 = int(np.max(x_coords)), int(np.max(y_coords))

                            center = ((x1 + x2) // 2, (y1 + y2) // 2)
                            area = (x2 - x1) * (y2 - y1)

                            spot = ParkingSpot(
                                id=len(parking_spots) + 1,
                                bbox=(x1, y1, x2, y2),
                                center=center,
                                area=area,
                                status=ParkingSpotStatus.UNKNOWN,
                                confidence=float(conf),
                                corners=[(int(x), int(y)) for x, y in corners]
                            )
                            parking_spots.append(spot)

                elif hasattr(result, 'boxes') and result.boxes is not None:
                    # ì¼ë°˜ bounding box ì²˜ë¦¬
                    for j, (box, conf, cls) in enumerate(zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls)):
                        if conf > 0.3:
                            x1, y1, x2, y2 = box.cpu().numpy().astype(int)
                            center = ((x1 + x2) // 2, (y1 + y2) // 2)
                            area = (x2 - x1) * (y2 - y1)

                            spot = ParkingSpot(
                                id=len(parking_spots) + 1,
                                bbox=(x1, y1, x2, y2),
                                center=center,
                                area=area,
                                status=ParkingSpotStatus.UNKNOWN,
                                confidence=float(conf)
                            )
                            parking_spots.append(spot)

            return parking_spots

        except Exception as e:
            print(f"âš ï¸ YOLO-OBB ê°ì§€ ì˜¤ë¥˜: {e}")
            return []

    def detect_with_contour_analysis(self, processed_images: Dict[str, np.ndarray]) -> List[ParkingSpot]:
        """ìœ¤ê³½ì„  ë¶„ì„ì„ í†µí•œ ì£¼ì°¨ ì˜ì—­ ê°ì§€"""
        # ì—¬ëŸ¬ ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì¡°í•©
        combined = cv2.bitwise_or(processed_images['canny'], processed_images['adaptive_threshold'])

        # í˜•íƒœí•™ì  ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
        combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel)
        combined = cv2.morphologyEx(combined, cv2.MORPH_OPEN, kernel)

        # ìœ¤ê³½ì„  ì°¾ê¸°
        contours, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        parking_spots = []
        min_area = 2000   # ìµœì†Œ ë©´ì  ì¦ê°€
        max_area = 40000  # ìµœëŒ€ ë©´ì  ì¡°ì •

        for i, contour in enumerate(contours):
            area = cv2.contourArea(contour)

            if min_area < area < max_area:
                # ìœ¤ê³½ì„ ì„ ì‚¬ê°í˜•ìœ¼ë¡œ ê·¼ì‚¬
                epsilon = 0.02 * cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, epsilon, True)

                # ë°”ìš´ë”© ë°•ìŠ¤
                x, y, w, h = cv2.boundingRect(contour)

                # ì¢…íš¡ë¹„ ê²€ì‚¬ (ì£¼ì°¨ ê³µê°„ì˜ ì¼ë°˜ì ì¸ ë¹„ìœ¨)
                aspect_ratio = w / h if h > 0 else 0
                if 0.7 < aspect_ratio < 2.5:  # ë” ì—„ê²©í•œ ë¹„ìœ¨ ì¡°ê±´
                    # ì»¨ë²¡ìŠ¤ í— ê²€ì‚¬ (ë„ˆë¬´ ë³µì¡í•œ í˜•íƒœ ì œì™¸)
                    hull = cv2.convexHull(contour)
                    hull_area = cv2.contourArea(hull)
                    solidity = area / hull_area if hull_area > 0 else 0

                    if solidity > 0.7:  # ì¶©ë¶„íˆ ë‹¨ìˆœí•œ í˜•íƒœë§Œ
                        center = (x + w // 2, y + h // 2)

                        spot = ParkingSpot(
                            id=len(parking_spots) + 1,
                            bbox=(x, y, x + w, y + h),
                            center=center,
                            area=area,
                            status=ParkingSpotStatus.UNKNOWN,
                            confidence=0.8,
                            corners=[(int(pt[0][0]), int(pt[0][1])) for pt in approx] if len(approx) <= 8 else None
                        )
                        parking_spots.append(spot)

        return parking_spots

    def detect_with_template_matching(self, image: np.ndarray, processed_images: Dict[str, np.ndarray]) -> List[ParkingSpot]:
        """í…œí”Œë¦¿ ë§¤ì¹­ì„ í†µí•œ ì£¼ì°¨ ì˜ì—­ ê°ì§€ (ê°œì„ ëœ ë²„ì „)"""
        gray = processed_images['enhanced']

        parking_spots = []

        # ë” í˜„ì‹¤ì ì¸ ì£¼ì°¨ ê³µê°„ í¬ê¸°ë“¤ (ì‹¤ì œ ì£¼ì°¨ì¥ ë¹„ìœ¨ ê³ ë ¤)
        template_sizes = [(90, 180), (100, 200), (80, 160)]

        for template_w, template_h in template_sizes:
            # ë” ì •êµí•œ í…œí”Œë¦¿ ìƒì„±
            template = np.ones((template_h, template_w), dtype=np.uint8) * 200

            # ì£¼ì°¨ì„  íŒ¨í„´ ì¶”ê°€
            cv2.rectangle(template, (0, 0), (template_w, 5), 100, -1)  # ìƒë‹¨ ì„ 
            cv2.rectangle(template, (0, template_h-5), (template_w, template_h), 100, -1)  # í•˜ë‹¨ ì„ 
            cv2.rectangle(template, (0, 0), (5, template_h), 100, -1)  # ì¢Œì¸¡ ì„ 
            cv2.rectangle(template, (template_w-5, 0), (template_w, template_h), 100, -1)  # ìš°ì¸¡ ì„ 

            template = cv2.GaussianBlur(template, (3, 3), 0)

            # í…œí”Œë¦¿ ë§¤ì¹­ (ë” ì—„ê²©í•œ ì„ê³„ê°’)
            result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF_NORMED)
            threshold = 0.75  # ë” ë†’ì€ ì„ê³„ê°’

            locations = np.where(result >= threshold)

            for pt in zip(*locations[::-1]):
                x, y = pt

                # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ê±°ë¦¬ ê²€ì‚¬
                is_duplicate = False
                for existing_spot in parking_spots:
                    ex, ey = existing_spot.center
                    distance = np.sqrt((x + template_w//2 - ex)**2 + (y + template_h//2 - ey)**2)
                    if distance < min(template_w, template_h) * 0.6:  # ë” ì—„ê²©í•œ ì¤‘ë³µ ê²€ì‚¬
                        is_duplicate = True
                        break

                if not is_duplicate:
                    center = (x + template_w // 2, y + template_h // 2)
                    confidence = float(result[y, x])

                    spot = ParkingSpot(
                        id=len(parking_spots) + 1,
                        bbox=(x, y, x + template_w, y + template_h),
                        center=center,
                        area=template_w * template_h,
                        status=ParkingSpotStatus.UNKNOWN,
                        confidence=confidence
                    )
                    parking_spots.append(spot)

                    # ë„ˆë¬´ ë§ì€ ê²°ê³¼ ë°©ì§€
                    if len(parking_spots) > 100:
                        return parking_spots

        return parking_spots    def merge_detection_results(self, *detection_results: List[List[ParkingSpot]]) -> List[ParkingSpot]:
        """ì—¬ëŸ¬ ê°ì§€ ê²°ê³¼ë¥¼ ë³‘í•©"""
        all_spots = []
        for spots in detection_results:
            all_spots.extend(spots)

        if not all_spots:
            return []

        # ê±°ë¦¬ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ë° ë³‘í•©
        merged_spots = []
        merge_threshold = 50  # ë³‘í•© ê±°ë¦¬ ì„ê³„ê°’

        for spot in all_spots:
            merged = False

            for existing_spot in merged_spots:
                # ì¤‘ì‹¬ì  ê°„ ê±°ë¦¬ ê³„ì‚°
                distance = np.sqrt(
                    (spot.center[0] - existing_spot.center[0])**2 +
                    (spot.center[1] - existing_spot.center[1])**2
                )

                if distance < merge_threshold:
                    # ë” ë†’ì€ ì‹ ë¢°ë„ì˜ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                    if spot.confidence > existing_spot.confidence:
                        existing_spot.bbox = spot.bbox
                        existing_spot.center = spot.center
                        existing_spot.area = spot.area
                        existing_spot.confidence = spot.confidence
                        if spot.corners:
                            existing_spot.corners = spot.corners
                    merged = True
                    break

            if not merged:
                spot.id = len(merged_spots) + 1
                merged_spots.append(spot)

        return merged_spots

    def analyze_parking_occupancy(self, image: np.ndarray, parking_spots: List[ParkingSpot]) -> List[ParkingSpot]:
        """ì£¼ì°¨ ìƒíƒœ ë¶„ì„ (ì ìœ /ë¹ˆìë¦¬)"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        for spot in parking_spots:
            x1, y1, x2, y2 = spot.bbox
            roi = gray[y1:y2, x1:x2]

            if roi.size > 0:
                # í†µê³„ì  ë¶„ì„
                mean_intensity = np.mean(roi)
                std_intensity = np.std(roi)

                # ì—£ì§€ ë°€ë„ ë¶„ì„
                edges = cv2.Canny(roi, 50, 150)
                edge_density = np.sum(edges > 0) / (roi.shape[0] * roi.shape[1])

                # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ (ì»¬ëŸ¬ ì´ë¯¸ì§€ì—ì„œ)
                roi_color = image[y1:y2, x1:x2]
                hist = cv2.calcHist([roi_color], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
                hist_complexity = np.std(hist)

                # ì ìœ  ìƒíƒœ íŒë‹¨ (íœ´ë¦¬ìŠ¤í‹±)
                if edge_density > 0.1 and hist_complexity > 100:
                    spot.status = ParkingSpotStatus.OCCUPIED
                    spot.confidence = min(spot.confidence + 0.2, 1.0)
                elif mean_intensity > 120 and std_intensity < 30:
                    spot.status = ParkingSpotStatus.EMPTY
                    spot.confidence = min(spot.confidence + 0.1, 1.0)
                else:
                    spot.status = ParkingSpotStatus.UNKNOWN

        return parking_spots

    def process_parking_lot(self, image_path: str) -> Tuple[np.ndarray, List[ParkingSpot], Dict]:
        """ì „ì²´ ì£¼ì°¨ì¥ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
        print(f"ğŸš— ì£¼ì°¨ì¥ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {image_path}")

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")

        print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")

        # ê³ ê¸‰ ì „ì²˜ë¦¬
        processed_images = self.preprocess_image_advanced(image)

        # ë‹¤ì¤‘ ê°ì§€ ë°©ë²• ì ìš©
        detection_results = []

        # 1. YOLO-OBB ê°ì§€
        if self.yolo_model:
            print("ğŸ” YOLO-OBB ê°ì§€ ì‹¤í–‰...")
            yolo_spots = self.detect_with_yolo_obb(image)
            detection_results.append(yolo_spots)
            print(f"   YOLO-OBB ê²°ê³¼: {len(yolo_spots)}ê°œ")

        # 2. ìœ¤ê³½ì„  ë¶„ì„
        print("ğŸ” ìœ¤ê³½ì„  ë¶„ì„ ì‹¤í–‰...")
        contour_spots = self.detect_with_contour_analysis(processed_images)
        detection_results.append(contour_spots)
        print(f"   ìœ¤ê³½ì„  ë¶„ì„ ê²°ê³¼: {len(contour_spots)}ê°œ")

        # 3. í…œí”Œë¦¿ ë§¤ì¹­
        print("ğŸ” í…œí”Œë¦¿ ë§¤ì¹­ ì‹¤í–‰...")
        template_spots = self.detect_with_template_matching(image, processed_images)
        detection_results.append(template_spots)
        print(f"   í…œí”Œë¦¿ ë§¤ì¹­ ê²°ê³¼: {len(template_spots)}ê°œ")

        # ê²°ê³¼ ë³‘í•©
        print("ğŸ”„ ê°ì§€ ê²°ê³¼ ë³‘í•©...")
        merged_spots = self.merge_detection_results(*detection_results)
        print(f"   ë³‘í•© í›„ ê²°ê³¼: {len(merged_spots)}ê°œ")

        # ì£¼ì°¨ ìƒíƒœ ë¶„ì„
        print("ğŸ“Š ì£¼ì°¨ ìƒíƒœ ë¶„ì„...")
        final_spots = self.analyze_parking_occupancy(image, merged_spots)

        # í†µê³„ ì •ë³´
        stats = {
            'total_spots': len(final_spots),
            'empty_spots': sum(1 for spot in final_spots if spot.status == ParkingSpotStatus.EMPTY),
            'occupied_spots': sum(1 for spot in final_spots if spot.status == ParkingSpotStatus.OCCUPIED),
            'unknown_spots': sum(1 for spot in final_spots if spot.status == ParkingSpotStatus.UNKNOWN),
            'avg_confidence': np.mean([spot.confidence for spot in final_spots]) if final_spots else 0,
            'detection_methods': {
                'yolo_obb': len(yolo_spots) if 'yolo_spots' in locals() else 0,
                'contour': len(contour_spots),
                'template': len(template_spots)
            }
        }

        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {stats['total_spots']}ê°œ ì£¼ì°¨êµ¬ì—­ ê°ì§€")

        return image, final_spots, stats

    def draw_results(self, image: np.ndarray, parking_spots: List[ParkingSpot]) -> np.ndarray:
        """ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°"""
        result = image.copy()

        for spot in parking_spots:
            # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì •
            if spot.status == ParkingSpotStatus.EMPTY:
                color = (0, 255, 0)  # ì´ˆë¡ìƒ‰
                status_text = "EMPTY"
            elif spot.status == ParkingSpotStatus.OCCUPIED:
                color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰
                status_text = "OCCUPIED"
            else:
                color = (0, 255, 255)  # ë…¸ë€ìƒ‰
                status_text = "UNKNOWN"

            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            x1, y1, x2, y2 = spot.bbox
            cv2.rectangle(result, (x1, y1), (x2, y2), color, 2)

            # OBB ì½”ë„ˆê°€ ìˆìœ¼ë©´ ë‹¤ê°í˜• ê·¸ë¦¬ê¸°
            if spot.corners and len(spot.corners) >= 3:
                pts = np.array(spot.corners, np.int32)
                pts = pts.reshape((-1, 1, 2))
                cv2.polylines(result, [pts], True, color, 2)

            # ì¤‘ì‹¬ì  í‘œì‹œ
            cv2.circle(result, spot.center, 5, color, -1)

            # í…ìŠ¤íŠ¸ ì •ë³´
            text = f"P{spot.id}: {status_text}"
            conf_text = f"Conf: {spot.confidence:.2f}"

            cv2.putText(result, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            cv2.putText(result, conf_text, (x1, y1-25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

        return result

    def save_results(self, image: np.ndarray, parking_spots: List[ParkingSpot],
                    stats: Dict, output_dir: str = "advanced_results"):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)

        # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
        result_image = self.draw_results(image, parking_spots)
        result_path = os.path.join(output_dir, "advanced_parking_result.jpg")
        cv2.imwrite(result_path, result_image)
        print(f"ğŸ’¾ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥: {result_path}")

        # JSON ê²°ê³¼ ì €ì¥ (NumPy íƒ€ì…ì„ Python ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜)
        spots_data = []
        for spot in parking_spots:
            spot_dict = {
                'id': int(spot.id),
                'bbox': [int(x) for x in spot.bbox],
                'center': [int(x) for x in spot.center],
                'area': float(spot.area),
                'status': spot.status.value,
                'confidence': float(spot.confidence),
                'corners': [[int(x), int(y)] for x, y in spot.corners] if spot.corners else None
            }
            spots_data.append(spot_dict)

        json_data = {
            'statistics': stats,
            'parking_spots': spots_data,
            'analysis_info': {
                'timestamp': str(pd.Timestamp.now()),
                'total_spots': len(parking_spots),
                'device_used': str(self.device)
            }
        }

        json_path = os.path.join(output_dir, "advanced_parking_analysis.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ë¶„ì„ ë°ì´í„° ì €ì¥: {json_path}")

        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥
        report_path = os.path.join(output_dir, "parking_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=== ê³ ê¸‰ ì£¼ì°¨ì¥ ë¶„ì„ ë¦¬í¬íŠ¸ ===\n")
            f.write(f"ì´ ì£¼ì°¨êµ¬ì—­: {stats['total_spots']}ê°œ\n")
            f.write(f"ë¹ˆ ìë¦¬: {stats['empty_spots']}ê°œ\n")
            f.write(f"ì ìœ ëœ ìë¦¬: {stats['occupied_spots']}ê°œ\n")
            f.write(f"ë¶ˆëª…í™•í•œ ìë¦¬: {stats['unknown_spots']}ê°œ\n")
            f.write(f"í‰ê·  ì‹ ë¢°ë„: {stats['avg_confidence']:.3f}\n\n")

            f.write("ê°ì§€ ë°©ë²•ë³„ ê²°ê³¼:\n")
            for method, count in stats['detection_methods'].items():
                f.write(f"  {method}: {count}ê°œ\n")

            f.write("\n=== ìƒì„¸ ì£¼ì°¨êµ¬ì—­ ì •ë³´ ===\n")
            for spot in parking_spots:
                f.write(f"êµ¬ì—­ {spot.id}: {spot.status.value.upper()} "
                       f"(ì‹ ë¢°ë„: {spot.confidence:.3f}, ë©´ì : {spot.area:.0f})\n")

        print(f"ğŸ’¾ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš— ê³ ê¸‰ ì£¼ì°¨ì¥ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 50)

    # YOLO-OBB ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    yolo_obb_path = "../../yolov8n-obb.pt"  # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ ëª¨ë¸ íŒŒì¼

    # ê°ì§€ê¸° ì´ˆê¸°í™”
    detector = AdvancedParkingDetector(yolo_obb_path)

    # ì´ë¯¸ì§€ ê²½ë¡œ
    image_path = "parkinglot1.jpg"

    if not os.path.exists(image_path):
        print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return

    try:
        # ì£¼ì°¨ì¥ ë¶„ì„ ì‹¤í–‰
        original_image, parking_spots, stats = detector.process_parking_lot(image_path)

        # ê²°ê³¼ ì €ì¥
        detector.save_results(original_image, parking_spots, stats)

        # ì‹œê°í™”
        result_image = detector.draw_results(original_image, parking_spots)

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 50)
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("=" * 50)
        print(f"ğŸ…¿ï¸  ì´ ì£¼ì°¨êµ¬ì—­: {stats['total_spots']}ê°œ")
        print(f"ğŸŸ¢ ë¹ˆ ìë¦¬: {stats['empty_spots']}ê°œ")
        print(f"ğŸ”´ ì ìœ ëœ ìë¦¬: {stats['occupied_spots']}ê°œ")
        print(f"ğŸŸ¡ ë¶ˆëª…í™•: {stats['unknown_spots']}ê°œ")
        print(f"ğŸ“ˆ í‰ê·  ì‹ ë¢°ë„: {stats['avg_confidence']:.1%}")

        # matplotlibìœ¼ë¡œ ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0, 0].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
        axes[0, 0].set_title("ì›ë³¸ ì´ë¯¸ì§€")
        axes[0, 0].axis('off')

        # ê²°ê³¼ ì´ë¯¸ì§€
        axes[0, 1].imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))
        axes[0, 1].set_title(f"ê°ì§€ ê²°ê³¼ ({stats['total_spots']}ê°œ êµ¬ì—­)")
        axes[0, 1].axis('off')

        # í†µê³„ ì°¨íŠ¸
        labels = ['ë¹ˆ ìë¦¬', 'ì ìœ ', 'ë¶ˆëª…í™•']
        sizes = [stats['empty_spots'], stats['occupied_spots'], stats['unknown_spots']]
        colors = ['lightgreen', 'lightcoral', 'lightyellow']

        axes[1, 0].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        axes[1, 0].set_title("ì£¼ì°¨ ìƒíƒœ ë¶„í¬")

        # ê°ì§€ ë°©ë²•ë³„ ê²°ê³¼
        methods = list(stats['detection_methods'].keys())
        counts = list(stats['detection_methods'].values())

        axes[1, 1].bar(methods, counts, color=['skyblue', 'lightpink', 'lightsteelblue'])
        axes[1, 1].set_title("ê°ì§€ ë°©ë²•ë³„ ê²°ê³¼")
        axes[1, 1].set_ylabel("ê°ì§€ëœ êµ¬ì—­ ìˆ˜")

        plt.tight_layout()
        plt.savefig("advanced_parking_analysis.png", dpi=150, bbox_inches='tight')
        plt.show()

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # pandas import ì¶”ê°€
    try:
        import pandas as pd
    except ImportError:
        # pandasê°€ ì—†ìœ¼ë©´ timestampë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        from datetime import datetime
        class pd:
            class Timestamp:
                @staticmethod
                def now():
                    return datetime.now().isoformat()

    main()