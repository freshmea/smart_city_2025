"""
YOLO-OBB ê¸°ë°˜ Perspective Transform ì£¼ì°¨ì¥ ë¶„ì„ ì‹œìŠ¤í…œ
ì°¨ëŸ‰ í¬ê¸° ê· ì¼ì„±ê³¼ perspective correctionì„ í™œìš©í•œ ì •ë°€ ê°ì§€
"""

import json
import os
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Tuple

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.cluster import DBSCAN
from ultralytics import YOLO
from ultralytics import YOLO


class ParkingSpotStatus(Enum):
    """ì£¼ì°¨ ìƒíƒœ"""
    EMPTY = "empty"
    OCCUPIED = "occupied"
    UNKNOWN = "unknown"

@dataclass
class Vehicle:
    """ì°¨ëŸ‰ ì •ë³´"""
    bbox: Tuple[int, int, int, int]
    obb: Optional[np.ndarray]  # Oriented Bounding Box
    confidence: float
    size: Tuple[float, float]  # (width, height)
    center: Tuple[int, int]

@dataclass
class ParkingSpot:
    """ì£¼ì°¨ êµ¬ì—­ ì •ë³´"""
    id: int
    bbox: Tuple[int, int, int, int]  # (x1, y1, x2, y2)
    center: Tuple[int, int]
    area: float
    status: ParkingSpotStatus
    confidence: float
    grid_position: Optional[Tuple[int, int]] = None  # (row, col)
    corners: Optional[List[Tuple[int, int]]] = None

class OptimizedParkingDetector:
    """YOLO-OBB ê¸°ë°˜ Perspective Transform ì£¼ì°¨ì¥ ê°ì§€ ì‹œìŠ¤í…œ"""

    def __init__(self, yolo_obb_path: Optional[str] = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")

        # YOLO-OBB ëª¨ë¸ ë¡œë“œ
        if yolo_obb_path and os.path.exists(yolo_obb_path):
            try:
                self.yolo_model = YOLO(yolo_obb_path)
                print(f"âœ… YOLO-OBB ëª¨ë¸ ë¡œë“œ: {yolo_obb_path}")
            except Exception as e:
                print(f"âš ï¸ YOLO-OBB ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.yolo_model = None
        else:
            self.yolo_model = None

        # í‘œì¤€ ì£¼ì°¨ êµ¬ì—­ í¬ê¸° ì„¤ì •
        self.standard_parking_width = 2.5  # 2.5m
        self.standard_parking_length = 5.0  # 5.0m
        self.size_tolerance = 0.25  # 25% í—ˆìš© ì˜¤ì°¨

    def detect_vehicles_with_obb(self, image: np.ndarray) -> List[Vehicle]:
        """YOLO-OBBë¡œ ì°¨ëŸ‰ ê°ì§€ ë° í¬ê¸° ì •ê·œí™”"""
        if self.yolo_model is None:
            return []

        try:
            # YOLO-OBB ì¶”ë¡ 
            results = self.yolo_model(image, verbose=False, conf=0.3)
            vehicles = []

            for result in results:
                # OBB ê²°ê³¼ ì²˜ë¦¬
                if hasattr(result, 'obb') and result.obb is not None:
                    for i, (obb, conf, cls) in enumerate(zip(result.obb.xyxyxyxy, result.obb.conf, result.obb.cls)):
                        # ì°¨ëŸ‰ í´ë˜ìŠ¤ë§Œ (ìë™ì°¨, íŠ¸ëŸ­, ë²„ìŠ¤ ë“±)
                        if int(cls) in [2, 3, 5, 7]:  # COCO í´ë˜ìŠ¤
                            # OBB ì ë“¤ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
                            obb_points = obb.cpu().numpy().reshape(-1, 2)

                            # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
                            x_coords = obb_points[:, 0]
                            y_coords = obb_points[:, 1]
                            x1, y1 = int(np.min(x_coords)), int(np.min(y_coords))
                            x2, y2 = int(np.max(x_coords)), int(np.max(y_coords))

                            # ì°¨ëŸ‰ í¬ê¸° ê³„ì‚° (OBBì˜ ì‹¤ì œ í¬ê¸°)
                            width = np.linalg.norm(obb_points[1] - obb_points[0])
                            height = np.linalg.norm(obb_points[2] - obb_points[1])

                            # ì¤‘ì‹¬ì 
                            center = (int((x1 + x2) / 2), int((y1 + y2) / 2))

                            vehicle = Vehicle(
                                bbox=(x1, y1, x2, y2),
                                obb=obb_points,
                                confidence=float(conf),
                                size=(float(width), float(height)),
                                center=center
                            )
                            vehicles.append(vehicle)

                # ì¼ë°˜ ë°•ìŠ¤ ê²°ê³¼ë„ ì²˜ë¦¬ (OBBê°€ ì—†ëŠ” ê²½ìš°)
                elif hasattr(result, 'boxes') and result.boxes is not None:
                    for box, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):
                        if int(cls) in [2, 3, 5, 7]:
                            x1, y1, x2, y2 = [int(x) for x in box.cpu().numpy()]
                            width = x2 - x1
                            height = y2 - y1
                            center = ((x1 + x2) // 2, (y1 + y2) // 2)

                            vehicle = Vehicle(
                                bbox=(x1, y1, x2, y2),
                                obb=None,
                                confidence=float(conf),
                                size=(float(width), float(height)),
                                center=center
                            )
                            vehicles.append(vehicle)

            print(f"ğŸš— YOLO-OBBë¡œ ê°ì§€ëœ ì°¨ëŸ‰: {len(vehicles)}ëŒ€")

            # ì°¨ëŸ‰ í¬ê¸° ì •ê·œí™” ë° í•„í„°ë§
            vehicles = self.normalize_vehicle_sizes(vehicles)

            return vehicles

        except Exception as e:
            print(f"âš ï¸ YOLO-OBB ê°ì§€ ì˜¤ë¥˜: {e}")
            return []

    def normalize_vehicle_sizes(self, vehicles: List[Vehicle]) -> List[Vehicle]:
        """ì°¨ëŸ‰ í¬ê¸° ì •ê·œí™” ë° ì´ìƒê°’ ì œê±° - ê±°ì˜ ë™ì¼í•œ í¬ê¸°ì˜ ì°¨ëŸ‰ë§Œ ìœ ì§€"""
        if len(vehicles) < 2:
            return vehicles

        # ì°¨ëŸ‰ í¬ê¸°ë“¤ ìˆ˜ì§‘
        sizes = [(v.size[0], v.size[1]) for v in vehicles]
        widths = [s[0] for s in sizes]
        heights = [s[1] for s in sizes]

        # ì¤‘ê°„ê°’ê³¼ í‰ê· ê°’ ê³„ì‚°
        median_width = np.median(widths)
        median_height = np.median(heights)
        mean_width = np.mean(widths)
        mean_height = np.mean(heights)

        print(f"ğŸ“ ì°¨ëŸ‰ í¬ê¸° í†µê³„:")
        print(f"  - í­: ì¤‘ê°„ê°’ {median_width:.1f}, í‰ê·  {mean_width:.1f}")
        print(f"  - ë†’ì´: ì¤‘ê°„ê°’ {median_height:.1f}, í‰ê·  {mean_height:.1f}")

        # ì •ê·œí™”ëœ ì°¨ëŸ‰ ëª©ë¡ (í¬ê¸°ê°€ ê±°ì˜ ë™ì¼í•œ ì°¨ëŸ‰ë§Œ)
        normalized_vehicles = []

        for vehicle in vehicles:
            w, h = vehicle.size

            # í¬ê¸°ê°€ ì¤‘ê°„ê°’ì˜ í—ˆìš© ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸ (ë” ì—„ê²©í•œ ê¸°ì¤€)
            width_ratio = abs(w - median_width) / median_width
            height_ratio = abs(h - median_height) / median_height

            if width_ratio <= self.size_tolerance and height_ratio <= self.size_tolerance:
                normalized_vehicles.append(vehicle)
                print(f"âœ… ì°¨ëŸ‰ ìœ ì§€: {w:.1f}x{h:.1f}")
            else:
                print(f"âŒ í¬ê¸° ì´ìƒê°’ ì œê±°: {w:.1f}x{h:.1f} (ê¸°ì¤€: {median_width:.1f}x{median_height:.1f})")

        print(f"ï¿½ ì •ê·œí™” í›„ ì°¨ëŸ‰: {len(normalized_vehicles)}ëŒ€ (ì›ë³¸: {len(vehicles)}ëŒ€)")
        return normalized_vehicles

    def estimate_perspective_transform(self, vehicles: List[Vehicle], image_shape: Tuple[int, int]) -> Optional[np.ndarray]:
        """ì°¨ëŸ‰ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ perspective transform ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        if len(vehicles) < 4:
            print("âš ï¸ Perspective transformì„ ìœ„í•œ ì°¨ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 4ëŒ€ í•„ìš”)")
            return None

        height, width = image_shape[:2]

        # ì°¨ëŸ‰ ì¤‘ì‹¬ì ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì°¨ì¥ í‰ë©´ ì¶”ì •
        vehicle_centers = np.array([v.center for v in vehicles], dtype=np.float32)

        try:
            # ì°¨ëŸ‰ë“¤ì´ ê²©ì íŒ¨í„´ì„ í˜•ì„±í•œë‹¤ê³  ê°€ì •í•˜ê³  ì‚¬ê°í˜• ëª¨ì„œë¦¬ ì°¾ê¸°

            # ê° ì°¨ëŸ‰ìœ¼ë¡œë¶€í„° ëª¨ì„œë¦¬ê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚°
            distances_to_corners = []

            for center in vehicle_centers:
                # ë„¤ ëª¨ì„œë¦¬ë¡œë¶€í„°ì˜ ê±°ë¦¬ í•© ê³„ì‚°
                distances = [
                    center[0] + center[1],  # ì¢Œìƒë‹¨ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬
                    (width - center[0]) + center[1],  # ìš°ìƒë‹¨ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬
                    center[0] + (height - center[1]),  # ì¢Œí•˜ë‹¨ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬
                    (width - center[0]) + (height - center[1])  # ìš°í•˜ë‹¨ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬
                ]
                distances_to_corners.append(distances)

            distances_to_corners = np.array(distances_to_corners)

            # ê° ëª¨ì„œë¦¬ì— ê°€ì¥ ê°€ê¹Œìš´ ì°¨ëŸ‰ ì°¾ê¸°
            top_left_idx = np.argmin(distances_to_corners[:, 0])
            top_right_idx = np.argmin(distances_to_corners[:, 1])
            bottom_left_idx = np.argmin(distances_to_corners[:, 2])
            bottom_right_idx = np.argmin(distances_to_corners[:, 3])

            # ëª¨ì„œë¦¬ ì ë“¤
            src_points = np.array([
                vehicle_centers[top_left_idx],
                vehicle_centers[top_right_idx],
                vehicle_centers[bottom_right_idx],
                vehicle_centers[bottom_left_idx]
            ], dtype=np.float32)

            # ëª©í‘œ ì‚¬ê°í˜• (ì •ë©´ì—ì„œ ë³¸ ëª¨ìŠµ)
            margin = 100
            dst_points = np.array([
                [margin, margin],
                [width - margin, margin],
                [width - margin, height - margin],
                [margin, height - margin]
            ], dtype=np.float32)

            # Perspective transform ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            transform_matrix = cv2.getPerspectiveTransform(src_points, dst_points)

            print("âœ… Perspective transform ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì™„ë£Œ")
            return transform_matrix

        except Exception as e:
            print(f"âš ï¸ Perspective transform ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None

    def apply_perspective_correction(self, image: np.ndarray, transform_matrix: np.ndarray) -> np.ndarray:
        """Perspective correction ì ìš©"""
        height, width = image.shape[:2]
        corrected = cv2.warpPerspective(image, transform_matrix, (width, height))
        return corrected

    def detect_uniform_parking_grid(self, corrected_image: np.ndarray,
                                  vehicles: List[Vehicle]) -> List[ParkingSpot]:
        """ë³´ì •ëœ ì´ë¯¸ì§€ì—ì„œ ê· ì¼í•œ ì£¼ì°¨ ê²©ì ìƒì„±"""
        height, width = corrected_image.shape[:2]

        if not vehicles:
            # ì°¨ëŸ‰ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²©ì ìƒì„±
            return self.generate_uniform_grid(corrected_image.shape)

        # ì°¨ëŸ‰ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²©ì íŒ¨í„´ ì¶”ì •
        vehicle_centers = [v.center for v in vehicles]

        # X, Y ì¢Œí‘œë³„ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ê²©ì ë¼ì¸ ì°¾ê¸°
        x_coords = [c[0] for c in vehicle_centers]
        y_coords = [c[1] for c in vehicle_centers]

        # DBSCANì„ ì‚¬ìš©í•˜ì—¬ ê²©ì ë¼ì¸ ì¶”ì •
        def cluster_coordinates(coords, eps=60):
            if len(coords) < 2:
                return sorted(coords)

            coords_array = np.array(coords).reshape(-1, 1)
            clustering = DBSCAN(eps=eps, min_samples=1).fit(coords_array)

            clusters = {}
            for i, label in enumerate(clustering.labels_):
                if label != -1:  # ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ ê²½ìš°
                    if label not in clusters:
                        clusters[label] = []
                    clusters[label].append(coords[i])

            # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ê°’ ê³„ì‚°
            cluster_centers = []
            for cluster_coords in clusters.values():
                cluster_centers.append(int(np.mean(cluster_coords)))

            return sorted(cluster_centers)

        # ê²©ì ë¼ì¸ ì¢Œí‘œ ê³„ì‚°
        grid_x_lines = cluster_coordinates(x_coords)
        grid_y_lines = cluster_coordinates(y_coords)

        print(f"ğŸ“ ê°ì§€ëœ ê²©ì: Xì¶• {len(grid_x_lines)}ê°œ, Yì¶• {len(grid_y_lines)}ê°œ ë¼ì¸")

        # ê²©ì ë¼ì¸ì´ ë¶€ì¡±í•˜ë©´ ê· ë“± ë¶„í• ë¡œ ë³´ì™„
        if len(grid_x_lines) < 3:
            cols = max(6, len(vehicles) // 2)
            grid_x_lines = [width * i // cols for i in range(cols + 1)]
        if len(grid_y_lines) < 3:
            rows = max(3, len(vehicles) // 3)
            grid_y_lines = [height * i // rows for i in range(rows + 1)]

        # ê· ì¼í•œ ì£¼ì°¨ êµ¬ì—­ ìƒì„±
        parking_spots = []
        spot_id = 1

        for i in range(len(grid_y_lines) - 1):
            for j in range(len(grid_x_lines) - 1):
                x1 = grid_x_lines[j]
                x2 = grid_x_lines[j + 1]
                y1 = grid_y_lines[i]
                y2 = grid_y_lines[i + 1]

                # ê· ì¼í•œ í¬ê¸° ê²€ì¦
                w, h = x2 - x1, y2 - y1
                if 80 < w < 250 and 120 < h < 350:  # í‘œì¤€ ì£¼ì°¨ ê³µê°„ í¬ê¸°
                    center = ((x1 + x2) // 2, (y1 + y2) // 2)
                    area = w * h

                    spot = ParkingSpot(
                        id=spot_id,
                        bbox=(x1, y1, x2, y2),
                        center=center,
                        area=area,
                        status=ParkingSpotStatus.UNKNOWN,
                        confidence=0.85,
                        grid_position=(i, j),
                        corners=[(x1, y1), (x2, y1), (x2, y2), (x1, y2)]
                    )
                    parking_spots.append(spot)
                    spot_id += 1

        print(f"ğŸ…¿ï¸ ìƒì„±ëœ ê· ì¼ ì£¼ì°¨ êµ¬ì—­: {len(parking_spots)}ê°œ")
        return parking_spots

    def generate_uniform_grid(self, image_shape: Tuple[int, int],
                            rows: int = 5, cols: int = 8) -> List[ParkingSpot]:
        """ê· ì¼í•œ ê¸°ë³¸ ê²©ì ìƒì„±"""
        height, width = image_shape[:2]

        # ì—¬ë°± ì„¤ì •
        margin_x = width // 12
        margin_y = height // 10

        effective_width = width - 2 * margin_x
        effective_height = height - 2 * margin_y

        # ê· ì¼í•œ ì£¼ì°¨ êµ¬ì—­ í¬ê¸° ê³„ì‚°
        spot_width = effective_width // cols
        spot_height = effective_height // rows

        parking_spots = []
        spot_id = 1

        for row in range(rows):
            for col in range(cols):
                x1 = margin_x + col * spot_width
                y1 = margin_y + row * spot_height
                x2 = x1 + spot_width - 8  # ì•½ê°„ì˜ ê°„ê²©
                y2 = y1 + spot_height - 8

                center = ((x1 + x2) // 2, (y1 + y2) // 2)
                area = (x2 - x1) * (y2 - y1)

                spot = ParkingSpot(
                    id=spot_id,
                    bbox=(x1, y1, x2, y2),
                    center=center,
                    area=area,
                    status=ParkingSpotStatus.UNKNOWN,
                    confidence=0.75,
                    grid_position=(row, col),
                    corners=[(x1, y1), (x2, y1), (x2, y2), (x1, y2)]
                )
                parking_spots.append(spot)
                spot_id += 1

        return parking_spots

    def analyze_occupancy_with_precision(self, vehicles: List[Vehicle],
                                       parking_spots: List[ParkingSpot]) -> List[ParkingSpot]:
        """ì •ë°€í•œ ID ê¸°ë°˜ ì£¼ì°¨ ì ìœ  ë¶„ì„"""

        # ê° ì°¨ëŸ‰ì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ ì£¼ì°¨ êµ¬ì—­ ì°¾ê¸°
        for vehicle in vehicles:
            v_x1, v_y1, v_x2, v_y2 = vehicle.bbox
            v_center = vehicle.center

            best_spot = None
            best_score = 0

            for spot in parking_spots:
                s_x1, s_y1, s_x2, s_y2 = spot.bbox

                # ê²¹ì¹˜ëŠ” ì˜ì—­ ê³„ì‚°
                overlap_x1 = max(v_x1, s_x1)
                overlap_y1 = max(v_y1, s_y1)
                overlap_x2 = min(v_x2, s_x2)
                overlap_y2 = min(v_y2, s_y2)

                if overlap_x1 < overlap_x2 and overlap_y1 < overlap_y2:
                    overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
                    vehicle_area = (v_x2 - v_x1) * (v_y2 - v_y1)
                    spot_area = (s_x2 - s_x1) * (s_y2 - s_y1)

                    # ì—¬ëŸ¬ ì§€í‘œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
                    vehicle_overlap_ratio = overlap_area / vehicle_area
                    spot_overlap_ratio = overlap_area / spot_area

                    # ì¤‘ì‹¬ì  ê±°ë¦¬
                    center_distance = np.sqrt((v_center[0] - spot.center[0])**2 +
                                            (v_center[1] - spot.center[1])**2)
                    max_distance = np.sqrt((s_x2 - s_x1)**2 + (s_y2 - s_y1)**2)
                    center_score = 1 - (center_distance / max_distance)

                    # ì¢…í•© ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
                    combined_score = (vehicle_overlap_ratio * 0.4 +
                                    spot_overlap_ratio * 0.4 +
                                    center_score * 0.2)

                    if combined_score > best_score and combined_score > 0.4:
                        best_score = combined_score
                        best_spot = spot

            # ê°€ì¥ ì í•©í•œ ì£¼ì°¨ êµ¬ì—­ì— ì ìœ  í‘œì‹œ
            if best_spot:
                best_spot.status = ParkingSpotStatus.OCCUPIED
                best_spot.confidence = min(1.0, best_spot.confidence + vehicle.confidence * 0.5)
                print(f"ğŸš— ì°¨ëŸ‰ì´ ì£¼ì°¨êµ¬ì—­ P{best_spot.id:02d}ì— ì£¼ì°¨ë¨ (ì ìˆ˜: {best_score:.2f})")

        # ë‚˜ë¨¸ì§€ëŠ” ë¹ˆ ìë¦¬ë¡œ ì„¤ì •
        for spot in parking_spots:
            if spot.status == ParkingSpotStatus.UNKNOWN:
                spot.status = ParkingSpotStatus.EMPTY

        return parking_spots

    def process_parking_lot_with_perspective(self, image_path: str) -> Tuple[np.ndarray, np.ndarray, List[ParkingSpot], Dict]:
        """Perspective transformì„ ì ìš©í•œ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print(f"ğŸš— YOLO-OBB + Perspective ì£¼ì°¨ì¥ ë¶„ì„ ì‹œì‘: {image_path}")

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")

        print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")

        # 1. ì°¨ëŸ‰ ê°ì§€ (YOLO-OBB)
        vehicles = self.detect_vehicles_with_obb(image)

        # 2. Perspective transform ê³„ì‚° ë° ì ìš©
        transform_matrix = self.estimate_perspective_transform(vehicles, image.shape)

        corrected_image = image.copy()
        transformed_vehicles = vehicles.copy()

        if transform_matrix is not None:
            # 3. Perspective correction ì ìš©
            corrected_image = self.apply_perspective_correction(image, transform_matrix)

            # ì°¨ëŸ‰ ì¢Œí‘œë„ ë³€í™˜
            transformed_vehicles = []
            for vehicle in vehicles:
                # ì°¨ëŸ‰ ì¤‘ì‹¬ì  ë³€í™˜
                center_point = np.array([[vehicle.center]], dtype=np.float32)
                transformed_center = cv2.perspectiveTransform(center_point, transform_matrix)
                new_center = (int(transformed_center[0][0][0]), int(transformed_center[0][0][1]))

                # ë°”ìš´ë”© ë°•ìŠ¤ ëª¨ì„œë¦¬ ë³€í™˜
                bbox_corners = np.array([
                    [[vehicle.bbox[0], vehicle.bbox[1]]],  # ì¢Œìƒë‹¨
                    [[vehicle.bbox[2], vehicle.bbox[1]]],  # ìš°ìƒë‹¨
                    [[vehicle.bbox[2], vehicle.bbox[3]]],  # ìš°í•˜ë‹¨
                    [[vehicle.bbox[0], vehicle.bbox[3]]]   # ì¢Œí•˜ë‹¨
                ], dtype=np.float32)

                transformed_corners = cv2.perspectiveTransform(bbox_corners, transform_matrix)

                # ìƒˆë¡œìš´ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
                x_coords = [int(p[0][0]) for p in transformed_corners]
                y_coords = [int(p[0][1]) for p in transformed_corners]
                new_bbox = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))

                # ë³€í™˜ëœ ì°¨ëŸ‰ ì •ë³´ ìƒì„±
                transformed_vehicle = Vehicle(
                    bbox=new_bbox,
                    obb=vehicle.obb,
                    confidence=vehicle.confidence,
                    size=vehicle.size,  # í¬ê¸°ëŠ” ìœ ì§€
                    center=new_center
                )
                transformed_vehicles.append(transformed_vehicle)

        # 4. ê· ì¼í•œ ì£¼ì°¨ ê²©ì ìƒì„±
        parking_spots = self.detect_uniform_parking_grid(corrected_image, transformed_vehicles)

        # 5. ì •ë°€í•œ ì ìœ  ìƒíƒœ ë¶„ì„
        parking_spots = self.analyze_occupancy_with_precision(transformed_vehicles, parking_spots)

        # 6. í†µê³„ ê³„ì‚°
        empty_count = sum(1 for spot in parking_spots if spot.status == ParkingSpotStatus.EMPTY)
        occupied_count = sum(1 for spot in parking_spots if spot.status == ParkingSpotStatus.OCCUPIED)

        stats = {
            'total_spots': len(parking_spots),
            'empty_spots': empty_count,
            'occupied_spots': occupied_count,
            'vehicles_detected': len(vehicles),
            'vehicles_normalized': len(transformed_vehicles),
            'occupancy_rate': occupied_count / len(parking_spots) * 100 if parking_spots else 0,
            'perspective_corrected': transform_matrix is not None,
            'uniform_grid': True,
            'average_spot_area': np.mean([spot.area for spot in parking_spots]) if parking_spots else 0,
            'size_tolerance': self.size_tolerance * 100  # í¼ì„¼íŠ¸ë¡œ í‘œì‹œ
        }

        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {stats}")

        return image, corrected_image, parking_spots, stats

    def draw_results_with_perspective(self, original_image: np.ndarray, corrected_image: np.ndarray,
                                    parking_spots: List[ParkingSpot], vehicles: Optional[List[Vehicle]] = None) -> Tuple[np.ndarray, np.ndarray]:
        """ì›ë³¸ê³¼ ë³´ì •ëœ ì´ë¯¸ì§€ ëª¨ë‘ì— ê²°ê³¼ ì‹œê°í™”"""

        # ì›ë³¸ ì´ë¯¸ì§€ ê²°ê³¼
        original_result = original_image.copy()

        # ë³´ì •ëœ ì´ë¯¸ì§€ ê²°ê³¼
        corrected_result = corrected_image.copy()

        # ì£¼ì°¨ êµ¬ì—­ ê·¸ë¦¬ê¸° (ë³´ì •ëœ ì´ë¯¸ì§€ì—)
        for spot in parking_spots:
            x1, y1, x2, y2 = spot.bbox

            # ìƒíƒœë³„ ìƒ‰ìƒ
            if spot.status == ParkingSpotStatus.EMPTY:
                color = (0, 255, 0)  # ë…¹ìƒ‰
                text = "EMPTY"
            elif spot.status == ParkingSpotStatus.OCCUPIED:
                color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰
                text = "OCCUPIED"
            else:
                color = (0, 255, 255)  # ë…¸ë€ìƒ‰
                text = "UNKNOWN"

            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(corrected_result, (x1, y1), (x2, y2), color, 2)

            # IDì™€ ìƒíƒœ í…ìŠ¤íŠ¸
            cv2.putText(corrected_result, f"P{spot.id:02d}", (x1+5, y1+20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            cv2.putText(corrected_result, text, (x1+5, y1+40),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

            # ê²©ì ìœ„ì¹˜ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
            if spot.grid_position:
                row, col = spot.grid_position
                cv2.putText(corrected_result, f"({row},{col})", (x1+5, y1+55),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)

        # ì°¨ëŸ‰ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        if vehicles:
            for i, vehicle in enumerate(vehicles):
                x1, y1, x2, y2 = vehicle.bbox
                cv2.rectangle(corrected_result, (x1, y1), (x2, y2), (255, 0, 255), 3)
                cv2.putText(corrected_result, f"V{i+1} ({vehicle.confidence:.2f})",
                           (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)

                # ì°¨ëŸ‰ í¬ê¸° ì •ë³´
                w, h = vehicle.size
                cv2.putText(corrected_result, f"{w:.0f}x{h:.0f}",
                           (x1, y1-25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 255), 1)

        return original_result, corrected_result

    def save_results_with_perspective(self, original_image: np.ndarray, corrected_image: np.ndarray,
                                    parking_spots: List[ParkingSpot], stats: Dict, vehicles: Optional[List[Vehicle]] = None,
                                    output_dir: str = "optimized_perspective_results"):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)

        # ê²°ê³¼ ì´ë¯¸ì§€
        original_result, corrected_result = self.draw_results_with_perspective(
            original_image, corrected_image, parking_spots, vehicles)

        cv2.imwrite(f"{output_dir}/optimized_original.jpg", original_result)
        cv2.imwrite(f"{output_dir}/optimized_corrected.jpg", corrected_result)

        # JSON ë°ì´í„°
        json_data = {
            'statistics': stats,
            'parking_spots': [
                {
                    'id': spot.id,
                    'bbox': [int(x) for x in spot.bbox],
                    'center': [int(x) for x in spot.center],
                    'area': float(spot.area),
                    'status': spot.status.value,
                    'confidence': float(spot.confidence),
                    'grid_position': spot.grid_position,
                    'corners': spot.corners
                }
                for spot in parking_spots
            ],
            'vehicles': [
                {
                    'bbox': [int(x) for x in vehicle.bbox],
                    'center': [int(x) for x in vehicle.center],
                    'size': [float(x) for x in vehicle.size],
                    'confidence': float(vehicle.confidence)
                }
                for vehicle in vehicles or []
            ]
        }

        with open(f"{output_dir}/optimized_analysis.json", 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ ìµœì í™”ëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_dir}/")

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (í˜¸í™˜ì„± ìœ ì§€)
    def process_parking_lot(self, image_path: str) -> Tuple[np.ndarray, List[ParkingSpot], Dict]:
        """ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        original, corrected, spots, stats = self.process_parking_lot_with_perspective(image_path)
        return corrected, spots, stats

    def draw_results(self, image: np.ndarray, parking_spots: List[ParkingSpot],
                    vehicles: Optional[List[Vehicle]] = None) -> np.ndarray:
        """ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        if vehicles:
            _, result = self.draw_results_with_perspective(image, image, parking_spots, vehicles)
        else:
            # ì°¨ëŸ‰ ì •ë³´ ì¬êµ¬ì„± (í˜¸í™˜ì„±)
            vehicle_list = []
            for spot in parking_spots:
                if spot.status == ParkingSpotStatus.OCCUPIED:
                    vehicle = Vehicle(
                        bbox=spot.bbox,
                        obb=None,
                        confidence=spot.confidence,
                        size=(float(spot.bbox[2] - spot.bbox[0]), float(spot.bbox[3] - spot.bbox[1])),
                        center=spot.center
                    )
                    vehicle_list.append(vehicle)
            _, result = self.draw_results_with_perspective(image, image, parking_spots, vehicle_list)
        return result

    def save_results(self, image: np.ndarray, parking_spots: List[ParkingSpot],
                    stats: Dict, output_dir: str = "optimized_results"):
        """ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        self.save_results_with_perspective(image, image, parking_spots, stats, None, output_dir)


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš— YOLO-OBB + Perspective Transform ì£¼ì°¨ì¥ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 60)

    # ëª¨ë¸ ê²½ë¡œ
    yolo_path = "../../yolov8n-obb.pt"

    # ê°ì§€ê¸° ì´ˆê¸°í™”
    detector = OptimizedParkingDetector(yolo_path)

    # ì´ë¯¸ì§€ ì²˜ë¦¬
    image_path = "parkinglot1.jpg"

    if not os.path.exists(image_path):
        print(f"âŒ ì´ë¯¸ì§€ ì—†ìŒ: {image_path}")
        return

    try:
        # ìƒˆë¡œìš´ perspective ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰
        original, corrected, spots, stats = detector.process_parking_lot_with_perspective(image_path)

        # ë³€í™˜ëœ ì°¨ëŸ‰ ì •ë³´ë„ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
        vehicles = detector.detect_vehicles_with_obb(corrected)

        # ê²°ê³¼ ì €ì¥
        detector.save_results_with_perspective(original, corrected, spots, stats, vehicles)

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š ìµœì¢… ê²°ê³¼ (YOLO-OBB + Perspective Transform)")
        print("=" * 60)
        print(f"ğŸ…¿ï¸  ì´ ì£¼ì°¨êµ¬ì—­: {stats['total_spots']}ê°œ")
        print(f"ğŸŸ¢ ë¹ˆ ìë¦¬: {stats['empty_spots']}ê°œ")
        print(f"ğŸ”´ ì ìœ ëœ ìë¦¬: {stats['occupied_spots']}ê°œ")
        print(f"ğŸš— ê°ì§€ëœ ì°¨ëŸ‰: {stats['vehicles_detected']}ëŒ€ â†’ {stats['vehicles_normalized']}ëŒ€ (ì •ê·œí™”)")
        print(f"ğŸ“ˆ ì ìœ ìœ¨: {stats['occupancy_rate']:.1f}%")
        print(f"ğŸ”§ Perspective ë³´ì •: {'âœ…' if stats['perspective_corrected'] else 'âŒ'}")
        print(f"ğŸ“ ê· ì¼ ê²©ì: {'âœ…' if stats['uniform_grid'] else 'âŒ'}")
        print(f"ğŸ“ í‰ê·  êµ¬ì—­ í¬ê¸°: {stats['average_spot_area']:.0f} í”½ì…€Â²")
        print(f"ğŸ¯ í¬ê¸° í—ˆìš© ì˜¤ì°¨: Â±{stats['size_tolerance']:.0f}%")

        # ì‹œê°í™”
        plt.figure(figsize=(20, 15))

        plt.subplot(3, 3, 1)
        plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))
        plt.title("1. ì›ë³¸ ì´ë¯¸ì§€", fontsize=12)
        plt.axis('off')

        plt.subplot(3, 3, 2)
        plt.imshow(cv2.cvtColor(corrected, cv2.COLOR_BGR2RGB))
        plt.title("2. Perspective ë³´ì •ëœ ì´ë¯¸ì§€", fontsize=12)
        plt.axis('off')

        # ê²°ê³¼ ì´ë¯¸ì§€ë“¤
        original_result, corrected_result = detector.draw_results_with_perspective(original, corrected, spots, vehicles)

        plt.subplot(3, 3, 3)
        plt.imshow(cv2.cvtColor(corrected_result, cv2.COLOR_BGR2RGB))
        plt.title(f"3. ë¶„ì„ ê²°ê³¼ ({stats['total_spots']}ê°œ êµ¬ì—­)", fontsize=12)
        plt.axis('off')

        plt.subplot(3, 3, 4)
        labels = ['ë¹ˆ ìë¦¬', 'ì ìœ ']
        sizes = [stats['empty_spots'], stats['occupied_spots']]
        colors = ['lightgreen', 'lightcoral']
        if sum(sizes) > 0:
            plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
        plt.title("4. ì£¼ì°¨ í˜„í™© ë¶„í¬", fontsize=12)

        plt.subplot(3, 3, 5)
        categories = ['ì´ êµ¬ì—­', 'ë¹ˆ ìë¦¬', 'ì ìœ ', 'ì°¨ëŸ‰(ì›ë³¸)', 'ì°¨ëŸ‰(ì •ê·œí™”)']
        values = [stats['total_spots'], stats['empty_spots'],
                 stats['occupied_spots'], stats['vehicles_detected'], stats['vehicles_normalized']]
        bars = plt.bar(categories, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'orange'])
        plt.title("5. í†µê³„ ìš”ì•½", fontsize=12)
        plt.ylabel("ê°œìˆ˜")
        plt.xticks(rotation=45)

        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar, value in zip(bars, values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,
                    str(value), ha='center', va='bottom')

        plt.subplot(3, 3, 6)
        # ì£¼ì°¨ êµ¬ì—­ë³„ ìƒíƒœ íˆíŠ¸ë§µ
        if spots and spots[0].grid_position:
            max_row = max(spot.grid_position[0] for spot in spots if spot.grid_position) + 1
            max_col = max(spot.grid_position[1] for spot in spots if spot.grid_position) + 1

            heatmap = np.zeros((max_row, max_col))
            for spot in spots:
                if spot.grid_position:
                    row, col = spot.grid_position
                    if spot.status == ParkingSpotStatus.OCCUPIED:
                        heatmap[row, col] = 1
                    elif spot.status == ParkingSpotStatus.EMPTY:
                        heatmap[row, col] = 0.5

            plt.imshow(heatmap, cmap='RdYlGn_r', aspect='auto')
            plt.title("6. ì£¼ì°¨ êµ¬ì—­ íˆíŠ¸ë§µ", fontsize=12)
            plt.xlabel("ì—´")
            plt.ylabel("í–‰")
            plt.colorbar(label="ì ìœ  ìƒíƒœ")

        # ì°¨ëŸ‰ í¬ê¸° ë¶„ì„
        if vehicles:
            plt.subplot(3, 3, 7)
            vehicle_areas = [v.size[0] * v.size[1] for v in vehicles]
            plt.hist(vehicle_areas, bins=10, alpha=0.7, color='purple')
            plt.title("7. ì°¨ëŸ‰ í¬ê¸° ë¶„í¬", fontsize=12)
            plt.xlabel("ë©´ì  (í”½ì…€Â²)")
            plt.ylabel("ë¹ˆë„")

        plt.subplot(3, 3, 8)
        # ì„±ëŠ¥ ì§€í‘œ
        performance_metrics = [
            'Perspective ë³´ì •',
            'ê· ì¼ ê²©ì',
            f'í¬ê¸° ì •ê·œí™” ({stats["size_tolerance"]:.0f}% í—ˆìš©)',
            f'ì ìœ ìœ¨ {stats["occupancy_rate"]:.1f}%'
        ]
        plt.text(0.1, 0.8, '\n'.join(performance_metrics), fontsize=12,
                verticalalignment='top', transform=plt.gca().transAxes)
        plt.title("8. ì‹œìŠ¤í…œ íŠ¹ì§•", fontsize=12)
        plt.axis('off')

        plt.subplot(3, 3, 9)
        # ì²˜ë¦¬ ë‹¨ê³„ ìš”ì•½
        process_steps = [
            '1. YOLO-OBB ì°¨ëŸ‰ ê°ì§€',
            '2. ì°¨ëŸ‰ í¬ê¸° ì •ê·œí™”',
            '3. Perspective Transform',
            '4. ê· ì¼ ê²©ì ìƒì„±',
            '5. ì •ë°€ ì ìœ  ë¶„ì„'
        ]
        plt.text(0.1, 0.9, '\n'.join(process_steps), fontsize=11,
                verticalalignment='top', transform=plt.gca().transAxes)
        plt.title("9. ì²˜ë¦¬ ê³¼ì •", fontsize=12)
        plt.axis('off')

        plt.tight_layout()
        plt.savefig("optimized_perspective_analysis.png", dpi=150, bbox_inches='tight')
        plt.show()

        # ì°¨ëŸ‰ í¬ê¸° ìƒì„¸ ë¶„ì„
        if vehicles:
            print("\n" + "=" * 60)
            print("ğŸš— ì°¨ëŸ‰ í¬ê¸° ë¶„ì„ ê²°ê³¼")
            print("=" * 60)
            widths = [v.size[0] for v in vehicles]
            heights = [v.size[1] for v in vehicles]

            print(f"ì°¨ëŸ‰ í­: í‰ê·  {np.mean(widths):.1f}px, í‘œì¤€í¸ì°¨ {np.std(widths):.1f}px")
            print(f"ì°¨ëŸ‰ ë†’ì´: í‰ê·  {np.mean(heights):.1f}px, í‘œì¤€í¸ì°¨ {np.std(heights):.1f}px")
            print(f"í¬ê¸° ê· ì¼ì„±: {100 - (np.std(widths) + np.std(heights)) / (np.mean(widths) + np.mean(heights)) * 100:.1f}%")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()