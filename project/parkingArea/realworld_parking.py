"""
ì‹¤ì œ ì£¼ì°¨ì¥ ì´ë¯¸ì§€ì— íŠ¹í™”ëœ ê³ ì •ë°€ ê°ì§€ ì‹œìŠ¤í…œ
ì»¬ëŸ¬ ì •ë³´, ê·¸ë¦¼ì, ì‹¤ì œ ì£¼ì°¨ì„  íŒ¨í„´ì„ ê³ ë ¤í•œ ì•Œê³ ë¦¬ì¦˜
"""

import json
import os
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Tuple

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.cluster import DBSCAN
from ultralytics import YOLO


class ParkingSpotStatus(Enum):
    EMPTY = "empty"
    OCCUPIED = "occupied"
    UNKNOWN = "unknown"

@dataclass
class ParkingSpot:
    id: int
    bbox: Tuple[int, int, int, int]
    center: Tuple[int, int]
    area: float
    status: ParkingSpotStatus
    confidence: float
    color_features: Optional[Dict] = None

class RealWorldParkingDetector:
    """ì‹¤ì œ ì£¼ì°¨ì¥ì— íŠ¹í™”ëœ ê³ ì •ë°€ ê°ì§€ ì‹œìŠ¤í…œ"""

    def __init__(self, yolo_path: str = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")

        # YOLO ëª¨ë¸ ë¡œë“œ
        if yolo_path and os.path.exists(yolo_path):
            try:
                self.yolo_model = YOLO(yolo_path)
                print(f"âœ… YOLO ëª¨ë¸ ë¡œë“œ: {yolo_path}")
            except:
                self.yolo_model = None
        else:
            self.yolo_model = None

    def advanced_preprocessing(self, image: np.ndarray) -> Dict[str, np.ndarray]:
        """ê³ ê¸‰ ì „ì²˜ë¦¬ with ì»¬ëŸ¬ ì •ë³´ í™œìš©"""
        results = {}

        # ì›ë³¸
        results['original'] = image.copy()

        # ìƒ‰ìƒ ê³µê°„ ë³€í™˜
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        results['hsv'] = hsv
        results['lab'] = lab
        results['gray'] = gray

        # ë°ê¸° ì •ê·œí™”
        normalized = cv2.equalizeHist(gray)
        results['normalized'] = normalized

        # CLAHE with ë” ê°•í•œ ì„¤ì •
        clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)
        results['enhanced'] = enhanced

        # ê·¸ë¦¼ì ì œê±° (LAB ìƒ‰ìƒ ê³µê°„ í™œìš©)
        l_channel = lab[:,:,0]
        shadow_removed = cv2.bilateralFilter(l_channel, 9, 75, 75)
        results['shadow_removed'] = shadow_removed

        # ë…¸ë©´ ìƒ‰ìƒ ê°ì§€ (HSVì—ì„œ íšŒìƒ‰/í°ìƒ‰ ë²”ìœ„)
        lower_road = np.array([0, 0, 100])
        upper_road = np.array([180, 30, 255])
        road_mask = cv2.inRange(hsv, lower_road, upper_road)
        results['road_mask'] = road_mask

        # ì£¼ì°¨ì„  ìƒ‰ìƒ ê°ì§€ (í°ìƒ‰/ë…¸ë€ìƒ‰)
        # í°ìƒ‰ ë²”ìœ„
        lower_white = np.array([0, 0, 200])
        upper_white = np.array([180, 30, 255])
        white_mask = cv2.inRange(hsv, lower_white, upper_white)

        # ë…¸ë€ìƒ‰ ë²”ìœ„
        lower_yellow = np.array([15, 50, 50])
        upper_yellow = np.array([35, 255, 255])
        yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)

        line_mask = cv2.bitwise_or(white_mask, yellow_mask)
        results['line_mask'] = line_mask

        # ì—£ì§€ ê²€ì¶œ (ë‹¤ì¤‘ ë°©ë²•)
        # Canny with ìë™ ì„ê³„ê°’
        v = np.median(enhanced)
        sigma = 0.33
        lower = int(max(0, (1.0 - sigma) * v))
        upper = int(min(255, (1.0 + sigma) * v))
        auto_canny = cv2.Canny(enhanced, lower, upper)
        results['auto_canny'] = auto_canny

        # Sobel í•„í„°
        sobelx = cv2.Sobel(enhanced, cv2.CV_64F, 1, 0, ksize=3)
        sobely = cv2.Sobel(enhanced, cv2.CV_64F, 0, 1, ksize=3)
        sobel_combined = np.sqrt(sobelx**2 + sobely**2)
        sobel_combined = np.uint8(sobel_combined / sobel_combined.max() * 255)
        results['sobel'] = sobel_combined

        # í˜•íƒœí•™ì  ì—°ì‚°
        kernel_h = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 1))  # ìˆ˜í‰ì„  ê°•ì¡°
        kernel_v = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 15))  # ìˆ˜ì§ì„  ê°•ì¡°

        horizontal_lines = cv2.morphologyEx(auto_canny, cv2.MORPH_OPEN, kernel_h)
        vertical_lines = cv2.morphologyEx(auto_canny, cv2.MORPH_OPEN, kernel_v)

        results['horizontal_morph'] = horizontal_lines
        results['vertical_morph'] = vertical_lines

        return results

    def detect_parking_lines_advanced(self, processed: Dict[str, np.ndarray]) -> Tuple[List, List]:
        """ê³ ê¸‰ ì£¼ì°¨ì„  ê°ì§€"""

        # ì£¼ì°¨ì„  ë§ˆìŠ¤í¬ì™€ í˜•íƒœí•™ì  ê²°ê³¼ ê²°í•©
        line_enhanced = cv2.bitwise_or(
            processed['line_mask'],
            cv2.bitwise_or(processed['horizontal_morph'], processed['vertical_morph'])
        )

        # ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        line_enhanced = cv2.morphologyEx(line_enhanced, cv2.MORPH_CLOSE, kernel)

        # ë‹¤ë‹¨ê³„ Hough ë³€í™˜
        all_lines = []

        # 1ë‹¨ê³„: ê°•í•œ ì„  ê°ì§€
        lines1 = cv2.HoughLinesP(
            line_enhanced, 1, np.pi/180, threshold=100,
            minLineLength=80, maxLineGap=20
        )
        if lines1 is not None:
            all_lines.extend(lines1)

        # 2ë‹¨ê³„: ì•½í•œ ì„  ê°ì§€ (ë” ë‚®ì€ ì„ê³„ê°’)
        lines2 = cv2.HoughLinesP(
            line_enhanced, 1, np.pi/180, threshold=50,
            minLineLength=40, maxLineGap=30
        )
        if lines2 is not None:
            all_lines.extend(lines2)

        # 3ë‹¨ê³„: ìˆ˜ì§ì„  íŠ¹í™” ê°ì§€ (ìˆ˜ì§ ë°©í–¥ ê°•ì¡°)
        vertical_enhanced = processed['vertical_morph']
        lines3 = cv2.HoughLinesP(
            vertical_enhanced, 1, np.pi/180, threshold=30,
            minLineLength=30, maxLineGap=40
        )
        if lines3 is not None:
            all_lines.extend(lines3)

        if not all_lines:
            return [], []

        # ì„  ë¶„ë¥˜ ë° í•„í„°ë§
        horizontal_lines = []
        vertical_lines = []

        for line in all_lines:
            x1, y1, x2, y2 = line[0]

            # ì„ ì˜ ê°ë„ì™€ ê¸¸ì´ ê³„ì‚°
            angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi
            length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

            # ìµœì†Œ ê¸¸ì´ í•„í„°
            if length < 20:
                continue

            # ê°ë„ë³„ ë¶„ë¥˜ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
            if abs(angle) <= 20 or abs(angle) >= 160:  # ìˆ˜í‰ì„ 
                horizontal_lines.append((x1, y1, x2, y2, length, angle))
            elif 70 <= abs(angle) <= 110:  # ìˆ˜ì§ì„ 
                vertical_lines.append((x1, y1, x2, y2, length, angle))

        # ê¸¸ì´ ê¸°ì¤€ ì •ë ¬ ë° í•„í„°ë§
        horizontal_lines.sort(key=lambda x: x[4], reverse=True)  # ê¸¸ì´ìˆœ
        vertical_lines.sort(key=lambda x: x[4], reverse=True)

        # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
        h_merged = self.merge_lines_advanced(horizontal_lines, is_horizontal=True)
        v_merged = self.merge_lines_advanced(vertical_lines, is_horizontal=False)

        print(f"ğŸ“ ê³ ê¸‰ ì„  ê°ì§€: ìˆ˜í‰ {len(h_merged)}ê°œ, ìˆ˜ì§ {len(v_merged)}ê°œ")

        return h_merged, v_merged

    def merge_lines_advanced(self, lines: List, is_horizontal: bool,
                           distance_threshold: int = 25, angle_threshold: float = 10) -> List:
        """ê³ ê¸‰ ì„  ë³‘í•© ì•Œê³ ë¦¬ì¦˜"""
        if not lines:
            return []

        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§ì„ ì‚¬ìš©í•œ ì„  ê·¸ë£¹í™”
        if is_horizontal:
            # ìˆ˜í‰ì„ ì€ y ì¢Œí‘œì™€ ê°ë„ë¡œ ê·¸ë£¹í™”
            features = np.array([[line[1], line[3], line[5]] for line in lines])  # y1, y2, angle
        else:
            # ìˆ˜ì§ì„ ì€ x ì¢Œí‘œì™€ ê°ë„ë¡œ ê·¸ë£¹í™”
            features = np.array([[line[0], line[2], line[5]] for line in lines])  # x1, x2, angle

        if len(features) == 0:
            return []

        # ì •ê·œí™”
        features_normalized = features.copy()
        features_normalized[:, :2] /= distance_threshold  # ê±°ë¦¬ ì •ê·œí™”
        features_normalized[:, 2] /= angle_threshold      # ê°ë„ ì •ê·œí™”

        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        clustering = DBSCAN(eps=1.0, min_samples=1).fit(features_normalized)
        labels = clustering.labels_

        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì„  ë³‘í•©
        merged_lines = []
        for cluster_id in set(labels):
            if cluster_id == -1:  # ë…¸ì´ì¦ˆ
                continue

            cluster_lines = [lines[i] for i in range(len(lines)) if labels[i] == cluster_id]

            # í´ëŸ¬ìŠ¤í„° ë‚´ ì„ ë“¤ì˜ í‰ê· ìœ¼ë¡œ ëŒ€í‘œì„  ìƒì„±
            avg_x1 = int(np.mean([line[0] for line in cluster_lines]))
            avg_y1 = int(np.mean([line[1] for line in cluster_lines]))
            avg_x2 = int(np.mean([line[2] for line in cluster_lines]))
            avg_y2 = int(np.mean([line[3] for line in cluster_lines]))

            # ê¸¸ì´ ì¬ê³„ì‚°
            length = np.sqrt((avg_x2 - avg_x1)**2 + (avg_y2 - avg_y1)**2)

            if length > 15:  # ìµœì†Œ ê¸¸ì´ ìœ ì§€
                merged_lines.append((avg_x1, avg_y1, avg_x2, avg_y2))

        return merged_lines

    def detect_parking_regions_by_color(self, image: np.ndarray, processed: Dict) -> List[ParkingSpot]:
        """ìƒ‰ìƒ ì •ë³´ë¥¼ í™œìš©í•œ ì£¼ì°¨ êµ¬ì—­ ê°ì§€"""

        # ì•„ìŠ¤íŒ”íŠ¸ ìƒ‰ìƒ ë§ˆìŠ¤í¬ (ì–´ë‘ìš´ íšŒìƒ‰)
        hsv = processed['hsv']
        lower_asphalt = np.array([0, 0, 20])
        upper_asphalt = np.array([180, 50, 120])
        asphalt_mask = cv2.inRange(hsv, lower_asphalt, upper_asphalt)

        # ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
        asphalt_mask = cv2.morphologyEx(asphalt_mask, cv2.MORPH_OPEN, kernel)
        asphalt_mask = cv2.morphologyEx(asphalt_mask, cv2.MORPH_CLOSE, kernel)

        # ìœ¤ê³½ì„  ì°¾ê¸°
        contours, _ = cv2.findContours(asphalt_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        parking_spots = []
        spot_id = 1

        for contour in contours:
            area = cv2.contourArea(contour)

            # ì£¼ì°¨ ê³µê°„ í¬ê¸° í•„í„°ë§
            if 2000 < area < 50000:

                # ìœ¤ê³½ì„  ê·¼ì‚¬
                epsilon = 0.02 * cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, epsilon, True)

                # ë°”ìš´ë”© ë°•ìŠ¤
                x, y, w, h = cv2.boundingRect(contour)

                # ì¢…íš¡ë¹„ ê²€ì‚¬
                aspect_ratio = w / h if h > 0 else 0
                if 0.5 < aspect_ratio < 4.0:

                    # ì»¨ë²¡ìŠ¤ì„± ê²€ì‚¬
                    hull = cv2.convexHull(contour)
                    hull_area = cv2.contourArea(hull)
                    solidity = area / hull_area if hull_area > 0 else 0

                    if solidity > 0.6:  # ì¶©ë¶„íˆ ë‹¨ìˆœí•œ í˜•íƒœ
                        center = (x + w // 2, y + h // 2)

                        # ìƒ‰ìƒ íŠ¹ì§• ì¶”ì¶œ
                        roi = image[y:y+h, x:x+w]
                        if roi.size > 0:
                            color_features = self.extract_color_features(roi)

                            spot = ParkingSpot(
                                id=spot_id,
                                bbox=(x, y, x + w, y + h),
                                center=center,
                                area=area,
                                status=ParkingSpotStatus.UNKNOWN,
                                confidence=0.7,
                                color_features=color_features
                            )
                            parking_spots.append(spot)
                            spot_id += 1

        return parking_spots

    def extract_color_features(self, roi: np.ndarray) -> Dict:
        """ì£¼ì°¨ êµ¬ì—­ì˜ ìƒ‰ìƒ íŠ¹ì§• ì¶”ì¶œ"""
        if roi.size == 0:
            return {}

        # HSV ë³€í™˜
        hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)

        # ìƒ‰ìƒ í†µê³„
        mean_color = np.mean(roi, axis=(0, 1))
        std_color = np.std(roi, axis=(0, 1))

        # HSV í†µê³„
        mean_hsv = np.mean(hsv_roi, axis=(0, 1))

        # ë°ê¸° ë¶„í¬
        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        brightness_hist, _ = np.histogram(gray_roi, bins=32, range=(0, 256))

        # ì—£ì§€ ë°€ë„
        edges = cv2.Canny(gray_roi, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size

        return {
            'mean_bgr': mean_color.tolist(),
            'std_bgr': std_color.tolist(),
            'mean_hsv': mean_hsv.tolist(),
            'brightness_hist': brightness_hist.tolist(),
            'edge_density': float(edge_density),
            'dominant_brightness': float(np.argmax(brightness_hist) * 8)  # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
        }

    def smart_grid_generation(self, image_shape: Tuple[int, int],
                            h_lines: List, v_lines: List,
                            color_regions: List[ParkingSpot]) -> List[ParkingSpot]:
        """ì§€ëŠ¥í˜• ê²©ì ìƒì„± (ì„ ê³¼ ìƒ‰ìƒ ì •ë³´ ê²°í•©)"""

        height, width = image_shape[:2]

        # 1. ì„  ê¸°ë°˜ ê²©ìê°€ ê°€ëŠ¥í•œ ê²½ìš°
        if len(h_lines) >= 2 and len(v_lines) >= 2:
            return self.generate_line_based_grid(h_lines, v_lines)

        # 2. ìƒ‰ìƒ ê¸°ë°˜ êµ¬ì—­ì´ ì¶©ë¶„í•œ ê²½ìš°
        elif len(color_regions) > 5:
            return self.refine_color_regions(color_regions, image_shape)

        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
        elif len(h_lines) >= 2 or len(v_lines) >= 1:
            return self.generate_hybrid_grid(image_shape, h_lines, v_lines, color_regions)

        # 4. ê¸°ë³¸ ì ì‘í˜• ê²©ì
        else:
            return self.generate_adaptive_grid(image_shape, color_regions)

    def generate_line_based_grid(self, h_lines: List, v_lines: List) -> List[ParkingSpot]:
        """ì„  ê¸°ë°˜ ê²©ì ìƒì„±"""
        parking_spots = []
        spot_id = 1

        # ì„ ë“¤ì„ ì •ë ¬
        h_sorted = sorted(h_lines, key=lambda x: (x[1] + x[3]) // 2)  # y í‰ê· ìœ¼ë¡œ ì •ë ¬
        v_sorted = sorted(v_lines, key=lambda x: (x[0] + x[2]) // 2)  # x í‰ê· ìœ¼ë¡œ ì •ë ¬

        for i in range(len(h_sorted) - 1):
            for j in range(len(v_sorted) - 1):
                # êµì°¨ì ìœ¼ë¡œ ì‚¬ê°í˜• ìƒì„±
                h1, h2 = h_sorted[i], h_sorted[i + 1]
                v1, v2 = v_sorted[j], v_sorted[j + 1]

                x1 = min(v1[0], v1[2])
                x2 = max(v2[0], v2[2])
                y1 = min(h1[1], h1[3])
                y2 = max(h2[1], h2[3])

                # í¬ê¸° ê²€ì¦
                w, h = x2 - x1, y2 - y1
                if 30 < w < 400 and 40 < h < 500:
                    center = ((x1 + x2) // 2, (y1 + y2) // 2)
                    area = w * h

                    spot = ParkingSpot(
                        id=spot_id,
                        bbox=(x1, y1, x2, y2),
                        center=center,
                        area=area,
                        status=ParkingSpotStatus.UNKNOWN,
                        confidence=0.9
                    )
                    parking_spots.append(spot)
                    spot_id += 1

        return parking_spots

    def generate_adaptive_grid(self, image_shape: Tuple[int, int],
                             color_regions: List[ParkingSpot]) -> List[ParkingSpot]:
        """ì ì‘í˜• ê²©ì ìƒì„±"""
        height, width = image_shape[:2]

        # ìƒ‰ìƒ êµ¬ì—­ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ê²©ì í¬ê¸° ì¶”ì •
        if color_regions:
            avg_width = np.mean([r.bbox[2] - r.bbox[0] for r in color_regions])
            avg_height = np.mean([r.bbox[3] - r.bbox[1] for r in color_regions])

            cols = max(3, int(width * 0.8 / avg_width))
            rows = max(2, int(height * 0.8 / avg_height))
        else:
            # ê¸°ë³¸ê°’
            cols = 6
            rows = 3

        # ì—¬ë°± ì„¤ì •
        margin_x = width // 10
        margin_y = height // 10

        effective_width = width - 2 * margin_x
        effective_height = height - 2 * margin_y

        spot_width = effective_width // cols
        spot_height = effective_height // rows

        parking_spots = []
        spot_id = 1

        for row in range(rows):
            for col in range(cols):
                x1 = margin_x + col * spot_width
                y1 = margin_y + row * spot_height
                x2 = x1 + spot_width - 15
                y2 = y1 + spot_height - 15

                center = ((x1 + x2) // 2, (y1 + y2) // 2)
                area = (x2 - x1) * (y2 - y1)

                spot = ParkingSpot(
                    id=spot_id,
                    bbox=(x1, y1, x2, y2),
                    center=center,
                    area=area,
                    status=ParkingSpotStatus.UNKNOWN,
                    confidence=0.6
                )
                parking_spots.append(spot)
                spot_id += 1

        return parking_spots

    def analyze_occupancy_advanced(self, image: np.ndarray, parking_spots: List[ParkingSpot],
                                 vehicles: List) -> List[ParkingSpot]:
        """ê³ ê¸‰ ì ìœ  ìƒíƒœ ë¶„ì„"""

        # 1. ì°¨ëŸ‰ ê¸°ë°˜ ì ìœ  íŒë‹¨
        for vehicle in vehicles:
            v_bbox = vehicle['bbox']
            v_center = ((v_bbox[0] + v_bbox[2]) // 2, (v_bbox[1] + v_bbox[3]) // 2)

            # ê²¹ì¹˜ëŠ” ì£¼ì°¨ êµ¬ì—­ ì°¾ê¸°
            for spot in parking_spots:
                if self.check_overlap(v_bbox, spot.bbox):
                    spot.status = ParkingSpotStatus.OCCUPIED
                    spot.confidence = min(1.0, spot.confidence + vehicle['confidence'] * 0.4)

        # 2. ìƒ‰ìƒ ê¸°ë°˜ ì ìœ  íŒë‹¨
        for spot in parking_spots:
            if spot.status != ParkingSpotStatus.OCCUPIED:
                occupancy_score = self.analyze_spot_occupancy_by_color(image, spot)

                if occupancy_score > 0.7:
                    spot.status = ParkingSpotStatus.OCCUPIED
                    spot.confidence = min(1.0, spot.confidence + occupancy_score * 0.3)
                elif occupancy_score < 0.3:
                    spot.status = ParkingSpotStatus.EMPTY
                else:
                    spot.status = ParkingSpotStatus.UNKNOWN

        return parking_spots

    def check_overlap(self, bbox1: Tuple, bbox2: Tuple, threshold: float = 0.3) -> bool:
        """ë‘ ë°•ìŠ¤ì˜ ê²¹ì¹¨ ì •ë„ í™•ì¸"""
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2

        # êµì§‘í•© ê³„ì‚°
        x1_inter = max(x1_1, x1_2)
        y1_inter = max(y1_1, y1_2)
        x2_inter = min(x2_1, x2_2)
        y2_inter = min(y2_1, y2_2)

        if x1_inter >= x2_inter or y1_inter >= y2_inter:
            return False

        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)

        # ë” ì‘ì€ ë°•ìŠ¤ ëŒ€ë¹„ ê²¹ì¹¨ ë¹„ìœ¨
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        smaller_area = min(area1, area2)

        overlap_ratio = inter_area / smaller_area if smaller_area > 0 else 0

        return overlap_ratio > threshold

    def analyze_spot_occupancy_by_color(self, image: np.ndarray, spot: ParkingSpot) -> float:
        """ìƒ‰ìƒ ë¶„ì„ì„ í†µí•œ ì ìœ  ì ìˆ˜ ê³„ì‚°"""
        x1, y1, x2, y2 = spot.bbox
        roi = image[y1:y2, x1:x2]

        if roi.size == 0:
            return 0.5

        # í˜„ì¬ ìƒ‰ìƒ íŠ¹ì§• ì¶”ì¶œ
        current_features = self.extract_color_features(roi)

        # ì ìœ  ê°€ëŠ¥ì„± ì ìˆ˜ ê³„ì‚°
        score = 0.0

        # 1. ë°ê¸° ë¶„ì‚° (ì°¨ëŸ‰ì´ ìˆìœ¼ë©´ ë” ë‹¤ì–‘í•œ ë°ê¸°)
        if 'brightness_hist' in current_features:
            hist = np.array(current_features['brightness_hist'])
            brightness_variance = np.var(hist)
            if brightness_variance > 1000:  # ì„ê³„ê°’ì€ ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì •
                score += 0.3

        # 2. ì—£ì§€ ë°€ë„ (ì°¨ëŸ‰ì´ ìˆìœ¼ë©´ ë” ë§ì€ ì—£ì§€)
        if current_features.get('edge_density', 0) > 0.1:
            score += 0.4

        # 3. ìƒ‰ìƒ ë³µì¡ì„± (ë¹ˆ ì•„ìŠ¤íŒ”íŠ¸ëŠ” ë‹¨ìˆœí•œ ìƒ‰ìƒ)
        if 'std_bgr' in current_features:
            color_complexity = np.mean(current_features['std_bgr'])
            if color_complexity > 20:
                score += 0.3

        return min(1.0, score)

    def process_parking_lot(self, image_path: str) -> Tuple[np.ndarray, List[ParkingSpot], Dict]:
        """ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print(f"ğŸš— ì‹¤ì œ ì£¼ì°¨ì¥ ë¶„ì„ ì‹œì‘: {image_path}")

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")

        print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")

        # 1. ê³ ê¸‰ ì „ì²˜ë¦¬
        processed = self.advanced_preprocessing(image)

        # 2. ì°¨ëŸ‰ ê°ì§€
        vehicles = []
        if self.yolo_model:
            try:
                results = self.yolo_model(image, verbose=False, conf=0.2)
                for result in results:
                    if hasattr(result, 'boxes') and result.boxes is not None:
                        for box, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):
                            if int(cls) in [2, 3, 5, 7]:  # ì°¨ëŸ‰ í´ë˜ìŠ¤
                                x1, y1, x2, y2 = [int(x) for x in box.cpu().numpy()]
                                vehicles.append({
                                    'bbox': (x1, y1, x2, y2),
                                    'confidence': float(conf),
                                    'class': int(cls)
                                })
            except Exception as e:
                print(f"âš ï¸ YOLO ê°ì§€ ì˜¤ë¥˜: {e}")

        print(f"ğŸš— ê°ì§€ëœ ì°¨ëŸ‰: {len(vehicles)}ëŒ€")

        # 3. ê³ ê¸‰ ì£¼ì°¨ì„  ê°ì§€
        h_lines, v_lines = self.detect_parking_lines_advanced(processed)

        # 4. ìƒ‰ìƒ ê¸°ë°˜ êµ¬ì—­ ê°ì§€
        color_regions = self.detect_parking_regions_by_color(image, processed)
        print(f"ğŸ¨ ìƒ‰ìƒ ê¸°ë°˜ êµ¬ì—­: {len(color_regions)}ê°œ")

        # 5. ì§€ëŠ¥í˜• ê²©ì ìƒì„±
        parking_spots = self.smart_grid_generation(image.shape, h_lines, v_lines, color_regions)
        print(f"ğŸ…¿ï¸ ìµœì¢… ì£¼ì°¨ êµ¬ì—­: {len(parking_spots)}ê°œ")

        # 6. ì ìœ  ìƒíƒœ ë¶„ì„
        parking_spots = self.analyze_occupancy_advanced(image, parking_spots, vehicles)

        # í†µê³„ ê³„ì‚°
        empty_count = sum(1 for spot in parking_spots if spot.status == ParkingSpotStatus.EMPTY)
        occupied_count = sum(1 for spot in parking_spots if spot.status == ParkingSpotStatus.OCCUPIED)
        unknown_count = sum(1 for spot in parking_spots if spot.status == ParkingSpotStatus.UNKNOWN)

        stats = {
            'total_spots': len(parking_spots),
            'empty_spots': empty_count,
            'occupied_spots': occupied_count,
            'unknown_spots': unknown_count,
            'vehicles_detected': len(vehicles),
            'horizontal_lines': len(h_lines),
            'vertical_lines': len(v_lines),
            'color_regions': len(color_regions),
            'occupancy_rate': occupied_count / len(parking_spots) * 100 if parking_spots else 0,
            'confidence_avg': np.mean([spot.confidence for spot in parking_spots]) if parking_spots else 0
        }

        print(f"âœ… ê³ ê¸‰ ë¶„ì„ ì™„ë£Œ")
        print(f"   ğŸ“Š ì ìœ : {occupied_count}ê°œ, ë¹ˆìë¦¬: {empty_count}ê°œ, ë¶ˆëª…: {unknown_count}ê°œ")
        print(f"   ğŸ“ˆ ì ìœ ìœ¨: {stats['occupancy_rate']:.1f}%, í‰ê·  ì‹ ë¢°ë„: {stats['confidence_avg']:.1%}")

        return image, parking_spots, stats

    def draw_results(self, image: np.ndarray, parking_spots: List[ParkingSpot],
                    vehicles: List = None) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™”"""
        result = image.copy()

        # ì£¼ì°¨ êµ¬ì—­ ê·¸ë¦¬ê¸°
        for spot in parking_spots:
            x1, y1, x2, y2 = spot.bbox

            # ìƒíƒœë³„ ìƒ‰ìƒ
            if spot.status == ParkingSpotStatus.EMPTY:
                color = (0, 255, 0)  # ë…¹ìƒ‰
                text = "EMPTY"
            elif spot.status == ParkingSpotStatus.OCCUPIED:
                color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰
                text = "OCCUPIED"
            else:
                color = (0, 255, 255)  # ë…¸ë€ìƒ‰
                text = "UNKNOWN"

            # ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì‹ ë¢°ë„ì— ë”°ë¥¸ ë‘ê»˜)
            thickness = max(1, int(spot.confidence * 3))
            cv2.rectangle(result, (x1, y1), (x2, y2), color, thickness)

            # í…ìŠ¤íŠ¸
            font_scale = 0.4
            cv2.putText(result, f"P{spot.id}", (x1+3, y1+15),
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, 1)
            cv2.putText(result, text, (x1+3, y1+30),
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, 1)
            cv2.putText(result, f"{spot.confidence:.2f}", (x1+3, y1+45),
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, 1)

        # ì°¨ëŸ‰ ê·¸ë¦¬ê¸°
        if vehicles:
            for vehicle in vehicles:
                x1, y1, x2, y2 = vehicle['bbox']
                cv2.rectangle(result, (x1, y1), (x2, y2), (255, 0, 255), 3)
                cv2.putText(result, f"Vehicle {vehicle['confidence']:.2f}",
                           (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)

        return result

    def save_results(self, image: np.ndarray, parking_spots: List[ParkingSpot],
                    stats: Dict, output_dir: str = "realworld_results"):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)

        # ê²°ê³¼ ì´ë¯¸ì§€
        result_image = self.draw_results(image, parking_spots)
        cv2.imwrite(f"{output_dir}/realworld_parking_result.jpg", result_image)

        # JSON ë°ì´í„°
        json_data = {
            'statistics': stats,
            'parking_spots': [
                {
                    'id': spot.id,
                    'bbox': [int(x) for x in spot.bbox],
                    'center': [int(x) for x in spot.center],
                    'area': float(spot.area),
                    'status': spot.status.value,
                    'confidence': float(spot.confidence),
                    'color_features': spot.color_features
                }
                for spot in parking_spots
            ]
        }

        with open(f"{output_dir}/realworld_analysis.json", 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ ì‹¤ì œ ì£¼ì°¨ì¥ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_dir}/")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš— ì‹¤ì œ ì£¼ì°¨ì¥ íŠ¹í™” ê³ ì •ë°€ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 50)

    # ëª¨ë¸ ê²½ë¡œ
    yolo_path = "../../yolov8n-obb.pt"

    # ê°ì§€ê¸° ì´ˆê¸°í™”
    detector = RealWorldParkingDetector(yolo_path)

    # ì´ë¯¸ì§€ ì²˜ë¦¬
    image_path = "parkinglot1.jpg"

    if not os.path.exists(image_path):
        print(f"âŒ ì´ë¯¸ì§€ ì—†ìŒ: {image_path}")
        return

    try:
        # ë¶„ì„ ì‹¤í–‰
        image, spots, stats = detector.process_parking_lot(image_path)

        # ì°¨ëŸ‰ ì¬ê°ì§€ (ì‹œê°í™”ìš©)
        vehicles = []
        if detector.yolo_model:
            try:
                results = detector.yolo_model(image, verbose=False, conf=0.2)
                for result in results:
                    if hasattr(result, 'boxes') and result.boxes is not None:
                        for box, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):
                            if int(cls) in [2, 3, 5, 7]:
                                x1, y1, x2, y2 = [int(x) for x in box.cpu().numpy()]
                                vehicles.append({
                                    'bbox': (x1, y1, x2, y2),
                                    'confidence': float(conf),
                                    'class': int(cls)
                                })
            except:
                pass

        # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
        result_image = detector.draw_results(image, spots, vehicles)

        # ê²°ê³¼ ì €ì¥
        detector.save_results(image, spots, stats)

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 50)
        print("ğŸ“Š ì‹¤ì œ ì£¼ì°¨ì¥ ë¶„ì„ ìµœì¢… ê²°ê³¼")
        print("=" * 50)
        print(f"ğŸ…¿ï¸  ì´ ì£¼ì°¨êµ¬ì—­: {stats['total_spots']}ê°œ")
        print(f"ğŸŸ¢ ë¹ˆ ìë¦¬: {stats['empty_spots']}ê°œ")
        print(f"ğŸ”´ ì ìœ ëœ ìë¦¬: {stats['occupied_spots']}ê°œ")
        print(f"ğŸŸ¡ ë¶ˆëª…í™•: {stats['unknown_spots']}ê°œ")
        print(f"ğŸš— ê°ì§€ëœ ì°¨ëŸ‰: {stats['vehicles_detected']}ëŒ€")
        print(f"ğŸ“ˆ ì ìœ ìœ¨: {stats['occupancy_rate']:.1f}%")
        print(f"ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {stats['confidence_avg']:.1%}")

        # ê³ ê¸‰ ì‹œê°í™”
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0, 0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        axes[0, 0].set_title("ì›ë³¸ ì´ë¯¸ì§€")
        axes[0, 0].axis('off')

        # ê²°ê³¼ ì´ë¯¸ì§€
        axes[0, 1].imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))
        axes[0, 1].set_title(f"ê³ ì •ë°€ ë¶„ì„ ê²°ê³¼ ({stats['total_spots']}ê°œ)")
        axes[0, 1].axis('off')

        # ì „ì²˜ë¦¬ ê²°ê³¼ (ì˜ˆì‹œ)
        processed = detector.advanced_preprocessing(image)
        axes[0, 2].imshow(processed['line_mask'], cmap='gray')
        axes[0, 2].set_title("ì£¼ì°¨ì„  ê°ì§€")
        axes[0, 2].axis('off')

        # í†µê³„ ì°¨íŠ¸ë“¤
        # ì£¼ì°¨ ìƒíƒœ íŒŒì´ ì°¨íŠ¸
        labels = ['ë¹ˆ ìë¦¬', 'ì ìœ ', 'ë¶ˆëª…í™•']
        sizes = [stats['empty_spots'], stats['occupied_spots'], stats['unknown_spots']]
        colors = ['lightgreen', 'lightcoral', 'lightyellow']

        axes[1, 0].pie([s for s in sizes if s > 0],
                      labels=[l for l, s in zip(labels, sizes) if s > 0],
                      colors=[c for c, s in zip(colors, sizes) if s > 0],
                      autopct='%1.1f%%', startangle=90)
        axes[1, 0].set_title("ì£¼ì°¨ ìƒíƒœ ë¶„í¬")

        # ê°ì§€ ë°©ë²•ë³„ ê²°ê³¼
        detection_methods = ['ìˆ˜í‰ì„ ', 'ìˆ˜ì§ì„ ', 'ìƒ‰ìƒêµ¬ì—­', 'ì°¨ëŸ‰']
        detection_counts = [stats['horizontal_lines'], stats['vertical_lines'],
                          stats['color_regions'], stats['vehicles_detected']]

        axes[1, 1].bar(detection_methods, detection_counts,
                      color=['skyblue', 'lightpink', 'lightsteelblue', 'gold'])
        axes[1, 1].set_title("ê°ì§€ ìš”ì†Œë³„ ê²°ê³¼")
        axes[1, 1].set_ylabel("ê°œìˆ˜")

        # ì„±ëŠ¥ ì§€í‘œ
        performance_labels = ['ì´ êµ¬ì—­', 'ê°ì§€ìœ¨', 'ì‹ ë¢°ë„', 'ì ìœ ìœ¨']
        performance_values = [stats['total_spots'],
                            (stats['empty_spots'] + stats['occupied_spots']) / stats['total_spots'] * 100,
                            stats['confidence_avg'] * 100,
                            stats['occupancy_rate']]

        axes[1, 2].bar(performance_labels, performance_values,
                      color=['purple', 'orange', 'green', 'red'])
        axes[1, 2].set_title("ì„±ëŠ¥ ì§€í‘œ")
        axes[1, 2].set_ylabel("ë¹„ìœ¨ (%)")

        plt.tight_layout()
        plt.savefig("realworld_parking_analysis.png", dpi=150, bbox_inches='tight')
        plt.show()

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()