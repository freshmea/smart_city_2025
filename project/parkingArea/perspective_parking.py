"""
Perspective Transform ê¸°ë°˜ ì •ë°€ ì£¼ì°¨ì¥ ë¶„ì„ ì‹œìŠ¤í…œ
YOLO-OBB ê²°ê³¼ë¥¼ í™œìš©í•œ perspective correctionê³¼ ê· ì¼í•œ ì£¼ì°¨ êµ¬ì—­ ì¸ì‹
"""

import json
import os
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Tuple

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.cluster import DBSCAN
from ultralytics import YOLO


class ParkingSpotStatus(Enum):
    EMPTY = "empty"
    OCCUPIED = "occupied"
    UNKNOWN = "unknown"

@dataclass
class Vehicle:
    """ì°¨ëŸ‰ ì •ë³´"""
    bbox: Tuple[int, int, int, int]
    obb: Optional[np.ndarray]  # Oriented Bounding Box
    confidence: float
    size: Tuple[float, float]  # (width, height)
    center: Tuple[int, int]

@dataclass
class ParkingSpot:
    """ì£¼ì°¨ êµ¬ì—­ ì •ë³´"""
    id: int
    bbox: Tuple[int, int, int, int]
    center: Tuple[int, int]
    area: float
    status: ParkingSpotStatus
    confidence: float
    grid_position: Optional[Tuple[int, int]] = None  # (row, col)
    corners: Optional[List[Tuple[int, int]]] = None

class PerspectiveParkingDetector:
    """Perspective Transform ê¸°ë°˜ ì£¼ì°¨ì¥ ê°ì§€ ì‹œìŠ¤í…œ"""

    def __init__(self, yolo_obb_path: str = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")

        # YOLO-OBB ëª¨ë¸ ë¡œë“œ
        if yolo_obb_path and os.path.exists(yolo_obb_path):
            try:
                self.yolo_model = YOLO(yolo_obb_path)
                print(f"âœ… YOLO-OBB ëª¨ë¸ ë¡œë“œ: {yolo_obb_path}")
            except Exception as e:
                print(f"âš ï¸ YOLO-OBB ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.yolo_model = None
        else:
            self.yolo_model = None

        # í‘œì¤€ ì£¼ì°¨ êµ¬ì—­ í¬ê¸° (ë¯¸í„° ë‹¨ìœ„)
        self.standard_parking_width = 2.5  # 2.5m
        self.standard_parking_length = 5.0  # 5.0m

        # í¬ê¸° í—ˆìš© ì˜¤ì°¨ (%)
        self.size_tolerance = 0.3  # 30%

    def detect_vehicles_with_obb(self, image: np.ndarray) -> List[Vehicle]:
        """YOLO-OBBë¡œ ì°¨ëŸ‰ ê°ì§€ ë° í¬ê¸° ì •ê·œí™”"""
        if self.yolo_model is None:
            return []

        try:
            # YOLO-OBB ì¶”ë¡ 
            results = self.yolo_model(image, verbose=False, conf=0.3)
            vehicles = []

            for result in results:
                # OBB ê²°ê³¼ ì²˜ë¦¬
                if hasattr(result, 'obb') and result.obb is not None:
                    for i, (obb, conf, cls) in enumerate(zip(result.obb.xyxyxyxy, result.obb.conf, result.obb.cls)):
                        # ì°¨ëŸ‰ í´ë˜ìŠ¤ë§Œ (ìë™ì°¨, íŠ¸ëŸ­, ë²„ìŠ¤ ë“±)
                        if int(cls) in [2, 3, 5, 7]:  # COCO í´ë˜ìŠ¤
                            # OBB ì ë“¤ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
                            obb_points = obb.cpu().numpy().reshape(-1, 2)

                            # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
                            x_coords = obb_points[:, 0]
                            y_coords = obb_points[:, 1]
                            x1, y1 = int(np.min(x_coords)), int(np.min(y_coords))
                            x2, y2 = int(np.max(x_coords)), int(np.max(y_coords))

                            # ì°¨ëŸ‰ í¬ê¸° ê³„ì‚° (OBBì˜ ì‹¤ì œ í¬ê¸°)
                            width = np.linalg.norm(obb_points[1] - obb_points[0])
                            height = np.linalg.norm(obb_points[2] - obb_points[1])

                            # ì¤‘ì‹¬ì 
                            center = (int((x1 + x2) / 2), int((y1 + y2) / 2))

                            vehicle = Vehicle(
                                bbox=(x1, y1, x2, y2),
                                obb=obb_points,
                                confidence=float(conf),
                                size=(float(width), float(height)),
                                center=center
                            )
                            vehicles.append(vehicle)

                # ì¼ë°˜ ë°•ìŠ¤ ê²°ê³¼ë„ ì²˜ë¦¬ (OBBê°€ ì—†ëŠ” ê²½ìš°)
                elif hasattr(result, 'boxes') and result.boxes is not None:
                    for box, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):
                        if int(cls) in [2, 3, 5, 7]:
                            x1, y1, x2, y2 = [int(x) for x in box.cpu().numpy()]
                            width = x2 - x1
                            height = y2 - y1
                            center = ((x1 + x2) // 2, (y1 + y2) // 2)

                            vehicle = Vehicle(
                                bbox=(x1, y1, x2, y2),
                                obb=None,
                                confidence=float(conf),
                                size=(float(width), float(height)),
                                center=center
                            )
                            vehicles.append(vehicle)

            print(f"ğŸš— YOLO-OBBë¡œ ê°ì§€ëœ ì°¨ëŸ‰: {len(vehicles)}ëŒ€")

            # ì°¨ëŸ‰ í¬ê¸° ì •ê·œí™” ë° í•„í„°ë§
            vehicles = self.normalize_vehicle_sizes(vehicles)

            return vehicles

        except Exception as e:
            print(f"âš ï¸ YOLO-OBB ê°ì§€ ì˜¤ë¥˜: {e}")
            return []

    def normalize_vehicle_sizes(self, vehicles: List[Vehicle]) -> List[Vehicle]:
        """ì°¨ëŸ‰ í¬ê¸° ì •ê·œí™” ë° ì´ìƒê°’ ì œê±°"""
        if len(vehicles) < 2:
            return vehicles

        # ì°¨ëŸ‰ í¬ê¸°ë“¤ ìˆ˜ì§‘
        sizes = [(v.size[0], v.size[1]) for v in vehicles]
        widths = [s[0] for s in sizes]
        heights = [s[1] for s in sizes]

        # ì¤‘ê°„ê°’ ê³„ì‚°
        median_width = np.median(widths)
        median_height = np.median(heights)

        print(f"ğŸ“ ì°¨ëŸ‰ í¬ê¸° ì¤‘ê°„ê°’: {median_width:.1f} x {median_height:.1f}")

        # í‘œì¤€ í¸ì°¨ ê³„ì‚°
        std_width = np.std(widths)
        std_height = np.std(heights)

        # ì •ê·œí™”ëœ ì°¨ëŸ‰ ëª©ë¡
        normalized_vehicles = []

        for vehicle in vehicles:
            w, h = vehicle.size

            # í¬ê¸°ê°€ ì¤‘ê°„ê°’ì˜ í—ˆìš© ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
            width_ratio = abs(w - median_width) / median_width
            height_ratio = abs(h - median_height) / median_height

            if width_ratio <= self.size_tolerance and height_ratio <= self.size_tolerance:
                normalized_vehicles.append(vehicle)
            else:
                print(f"âš ï¸ í¬ê¸° ì´ìƒê°’ ì œê±°: {w:.1f}x{h:.1f} (ê¸°ì¤€: {median_width:.1f}x{median_height:.1f})")

        print(f"âœ… ì •ê·œí™” í›„ ì°¨ëŸ‰: {len(normalized_vehicles)}ëŒ€")
        return normalized_vehicles

    def estimate_perspective_transform(self, vehicles: List[Vehicle], image_shape: Tuple[int, int]) -> Optional[np.ndarray]:
        """ì°¨ëŸ‰ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ perspective transform ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        if len(vehicles) < 4:
            print("âš ï¸ Perspective transformì„ ìœ„í•œ ì°¨ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 4ëŒ€ í•„ìš”)")
            return None

        height, width = image_shape[:2]

        # ì°¨ëŸ‰ ì¤‘ì‹¬ì ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì°¨ì¥ í‰ë©´ ì¶”ì •
        vehicle_centers = np.array([v.center for v in vehicles], dtype=np.float32)

        try:
            # ì°¨ëŸ‰ë“¤ì´ ê²©ì íŒ¨í„´ì„ í˜•ì„±í•œë‹¤ê³  ê°€ì •
            # ê°€ì¥ ë°”ê¹¥ìª½ 4ê°œ ì ì„ ì°¾ì•„ì„œ ì‚¬ê°í˜• í˜•ì„±

            # ê·¹ê°’ ì°¾ê¸°
            min_x_idx = np.argmin(vehicle_centers[:, 0])
            max_x_idx = np.argmax(vehicle_centers[:, 0])
            min_y_idx = np.argmin(vehicle_centers[:, 1])
            max_y_idx = np.argmax(vehicle_centers[:, 1])

            # ëª¨ì„œë¦¬ ì ë“¤ ì¶”ì •
            corners = []

            # ì¢Œìƒë‹¨: ìµœì†Œ x, ìµœì†Œ y ê·¼ì²˜
            top_left = vehicle_centers[np.argmin(vehicle_centers[:, 0] + vehicle_centers[:, 1])]

            # ìš°ìƒë‹¨: ìµœëŒ€ x, ìµœì†Œ y ê·¼ì²˜
            top_right = vehicle_centers[np.argmin(-vehicle_centers[:, 0] + vehicle_centers[:, 1])]

            # ì¢Œí•˜ë‹¨: ìµœì†Œ x, ìµœëŒ€ y ê·¼ì²˜
            bottom_left = vehicle_centers[np.argmin(vehicle_centers[:, 0] - vehicle_centers[:, 1])]

            # ìš°í•˜ë‹¨: ìµœëŒ€ x, ìµœëŒ€ y ê·¼ì²˜
            bottom_right = vehicle_centers[np.argmax(vehicle_centers[:, 0] + vehicle_centers[:, 1])]

            src_points = np.array([top_left, top_right, bottom_right, bottom_left], dtype=np.float32)

            # ëª©í‘œ ì‚¬ê°í˜• (ì •ë©´ì—ì„œ ë³¸ ëª¨ìŠµ)
            margin = 50
            dst_points = np.array([
                [margin, margin],
                [width - margin, margin],
                [width - margin, height - margin],
                [margin, height - margin]
            ], dtype=np.float32)

            # Perspective transform ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            transform_matrix = cv2.getPerspectiveTransform(src_points, dst_points)

            print("âœ… Perspective transform ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì™„ë£Œ")
            return transform_matrix

        except Exception as e:
            print(f"âš ï¸ Perspective transform ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None

    def apply_perspective_correction(self, image: np.ndarray, transform_matrix: np.ndarray) -> np.ndarray:
        """Perspective correction ì ìš©"""
        height, width = image.shape[:2]
        corrected = cv2.warpPerspective(image, transform_matrix, (width, height))
        return corrected

    def detect_parking_grid_from_corrected_image(self, corrected_image: np.ndarray,
                                               vehicles: List[Vehicle]) -> List[ParkingSpot]:
        """ë³´ì •ëœ ì´ë¯¸ì§€ì—ì„œ ê· ì¼í•œ ì£¼ì°¨ ê²©ì ìƒì„±"""
        height, width = corrected_image.shape[:2]

        if not vehicles:
            # ì°¨ëŸ‰ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²©ì ìƒì„±
            return self.generate_default_uniform_grid(corrected_image.shape)

        # ì°¨ëŸ‰ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²©ì íŒ¨í„´ ì¶”ì •
        vehicle_centers = [v.center for v in vehicles]

        # X, Y ì¢Œí‘œë³„ í´ëŸ¬ìŠ¤í„°ë§
        x_coords = [c[0] for c in vehicle_centers]
        y_coords = [c[1] for c in vehicle_centers]

        # DBSCANì„ ì‚¬ìš©í•˜ì—¬ ê²©ì ë¼ì¸ ì¶”ì •
        def cluster_coordinates(coords, min_samples=1):
            coords_array = np.array(coords).reshape(-1, 1)
            clustering = DBSCAN(eps=50, min_samples=min_samples).fit(coords_array)

            clusters = {}
            for i, label in enumerate(clustering.labels_):
                if label != -1:  # ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ ê²½ìš°
                    if label not in clusters:
                        clusters[label] = []
                    clusters[label].append(coords[i])

            # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ê°’ ê³„ì‚°
            cluster_centers = []
            for cluster_coords in clusters.values():
                cluster_centers.append(int(np.mean(cluster_coords)))

            return sorted(cluster_centers)

        # ê²©ì ë¼ì¸ ì¢Œí‘œ ê³„ì‚°
        grid_x_lines = cluster_coordinates(x_coords)
        grid_y_lines = cluster_coordinates(y_coords)

        print(f"ğŸ“ ê°ì§€ëœ ê²©ì: Xì¶• {len(grid_x_lines)}ê°œ, Yì¶• {len(grid_y_lines)}ê°œ ë¼ì¸")

        # ê²©ìê°€ ë¶€ì¡±í•˜ë©´ ë³´ì™„
        if len(grid_x_lines) < 3:
            grid_x_lines = list(range(width//8, width, width//4))
        if len(grid_y_lines) < 3:
            grid_y_lines = list(range(height//6, height, height//3))

        # ì£¼ì°¨ êµ¬ì—­ ìƒì„±
        parking_spots = []
        spot_id = 1

        for i in range(len(grid_y_lines) - 1):
            for j in range(len(grid_x_lines) - 1):
                x1 = grid_x_lines[j]
                x2 = grid_x_lines[j + 1]
                y1 = grid_y_lines[i]
                y2 = grid_y_lines[i + 1]

                # í¬ê¸° ê²€ì¦ (ê· ì¼í•œ í¬ê¸°)
                w, h = x2 - x1, y2 - y1
                if 80 < w < 200 and 120 < h < 300:  # ì ì ˆí•œ ì£¼ì°¨ ê³µê°„ í¬ê¸°
                    center = ((x1 + x2) // 2, (y1 + y2) // 2)
                    area = w * h

                    spot = ParkingSpot(
                        id=spot_id,
                        bbox=(x1, y1, x2, y2),
                        center=center,
                        area=area,
                        status=ParkingSpotStatus.UNKNOWN,
                        confidence=0.8,
                        grid_position=(i, j),
                        corners=[(x1, y1), (x2, y1), (x2, y2), (x1, y2)]
                    )
                    parking_spots.append(spot)
                    spot_id += 1

        print(f"ğŸ…¿ï¸ ìƒì„±ëœ ê· ì¼ ì£¼ì°¨ êµ¬ì—­: {len(parking_spots)}ê°œ")
        return parking_spots

    def generate_default_uniform_grid(self, image_shape: Tuple[int, int],
                                    rows: int = 6, cols: int = 10) -> List[ParkingSpot]:
        """ê¸°ë³¸ ê· ì¼ ê²©ì ìƒì„±"""
        height, width = image_shape[:2]

        # ì—¬ë°± ì„¤ì •
        margin_x = width // 10
        margin_y = height // 8

        effective_width = width - 2 * margin_x
        effective_height = height - 2 * margin_y

        # ê· ì¼í•œ ì£¼ì°¨ êµ¬ì—­ í¬ê¸° ê³„ì‚°
        spot_width = effective_width // cols
        spot_height = effective_height // rows

        parking_spots = []
        spot_id = 1

        for row in range(rows):
            for col in range(cols):
                x1 = margin_x + col * spot_width
                y1 = margin_y + row * spot_height
                x2 = x1 + spot_width - 5  # ì•½ê°„ì˜ ê°„ê²©
                y2 = y1 + spot_height - 5

                center = ((x1 + x2) // 2, (y1 + y2) // 2)
                area = (x2 - x1) * (y2 - y1)

                spot = ParkingSpot(
                    id=spot_id,
                    bbox=(x1, y1, x2, y2),
                    center=center,
                    area=area,
                    status=ParkingSpotStatus.UNKNOWN,
                    confidence=0.7,
                    grid_position=(row, col),
                    corners=[(x1, y1), (x2, y1), (x2, y2), (x1, y2)]
                )
                parking_spots.append(spot)
                spot_id += 1

        return parking_spots

    def analyze_occupancy_with_id(self, vehicles: List[Vehicle],
                                parking_spots: List[ParkingSpot]) -> List[ParkingSpot]:
        """ID ê¸°ë°˜ ì •í™•í•œ ì£¼ì°¨ ì ìœ  ë¶„ì„"""

        # ê° ì°¨ëŸ‰ì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ ì£¼ì°¨ êµ¬ì—­ ì°¾ê¸°
        for vehicle in vehicles:
            v_x1, v_y1, v_x2, v_y2 = vehicle.bbox
            v_center = vehicle.center

            best_spot = None
            best_overlap_ratio = 0

            for spot in parking_spots:
                s_x1, s_y1, s_x2, s_y2 = spot.bbox

                # ê²¹ì¹˜ëŠ” ì˜ì—­ ê³„ì‚°
                overlap_x1 = max(v_x1, s_x1)
                overlap_y1 = max(v_y1, s_y1)
                overlap_x2 = min(v_x2, s_x2)
                overlap_y2 = min(v_y2, s_y2)

                if overlap_x1 < overlap_x2 and overlap_y1 < overlap_y2:
                    overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
                    vehicle_area = (v_x2 - v_x1) * (v_y2 - v_y1)
                    spot_area = (s_x2 - s_x1) * (s_y2 - s_y1)

                    # ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚° (ì°¨ëŸ‰ ê¸°ì¤€ + ì£¼ì°¨êµ¬ì—­ ê¸°ì¤€)
                    vehicle_overlap_ratio = overlap_area / vehicle_area
                    spot_overlap_ratio = overlap_area / spot_area

                    # ì–‘ë°©í–¥ ê²¹ì¹¨ ë¹„ìœ¨ì˜ í‰ê· 
                    combined_ratio = (vehicle_overlap_ratio + spot_overlap_ratio) / 2

                    if combined_ratio > best_overlap_ratio and combined_ratio > 0.3:
                        best_overlap_ratio = combined_ratio
                        best_spot = spot

            # ê°€ì¥ ì í•©í•œ ì£¼ì°¨ êµ¬ì—­ì— ì ìœ  í‘œì‹œ
            if best_spot:
                best_spot.status = ParkingSpotStatus.OCCUPIED
                best_spot.confidence = min(1.0, best_spot.confidence + vehicle.confidence * 0.4)
                print(f"ğŸš— ì°¨ëŸ‰ì´ ì£¼ì°¨êµ¬ì—­ P{best_spot.id}ì— ì£¼ì°¨ë¨ (ê²¹ì¹¨: {best_overlap_ratio:.2f})")

        # ë‚˜ë¨¸ì§€ëŠ” ë¹ˆ ìë¦¬ë¡œ ì„¤ì •
        for spot in parking_spots:
            if spot.status == ParkingSpotStatus.UNKNOWN:
                spot.status = ParkingSpotStatus.EMPTY

        return parking_spots

    def process_parking_lot_with_perspective(self, image_path: str) -> Tuple[np.ndarray, np.ndarray, List[ParkingSpot], Dict]:
        """Perspective transformì„ ì ìš©í•œ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print(f"ğŸš— Perspective ê¸°ë°˜ ì£¼ì°¨ì¥ ë¶„ì„ ì‹œì‘: {image_path}")

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")

        print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")

        # 1. ì°¨ëŸ‰ ê°ì§€ (YOLO-OBB)
        vehicles = self.detect_vehicles_with_obb(image)

        # 2. Perspective transform ê³„ì‚°
        transform_matrix = self.estimate_perspective_transform(vehicles, image.shape)

        corrected_image = image.copy()
        if transform_matrix is not None:
            # 3. Perspective correction ì ìš©
            corrected_image = self.apply_perspective_correction(image, transform_matrix)

            # ì°¨ëŸ‰ ì¢Œí‘œë„ ë³€í™˜
            transformed_vehicles = []
            for vehicle in vehicles:
                # ì°¨ëŸ‰ ì¤‘ì‹¬ì  ë³€í™˜
                center_point = np.array([[vehicle.center]], dtype=np.float32)
                transformed_center = cv2.perspectiveTransform(center_point, transform_matrix)
                new_center = (int(transformed_center[0][0][0]), int(transformed_center[0][0][1]))

                # ë°”ìš´ë”© ë°•ìŠ¤ ë³€í™˜
                bbox_points = np.array([
                    [[vehicle.bbox[0], vehicle.bbox[1]]],
                    [[vehicle.bbox[2], vehicle.bbox[3]]]
                ], dtype=np.float32)
                transformed_bbox = cv2.perspectiveTransform(bbox_points, transform_matrix)

                new_bbox = (
                    int(transformed_bbox[0][0][0]), int(transformed_bbox[0][0][1]),
                    int(transformed_bbox[1][0][0]), int(transformed_bbox[1][0][1])
                )

                # ë³€í™˜ëœ ì°¨ëŸ‰ ì •ë³´ ìƒì„±
                transformed_vehicle = Vehicle(
                    bbox=new_bbox,
                    obb=vehicle.obb,
                    confidence=vehicle.confidence,
                    size=vehicle.size,
                    center=new_center
                )
                transformed_vehicles.append(transformed_vehicle)

            vehicles = transformed_vehicles

        # 4. ê· ì¼í•œ ì£¼ì°¨ ê²©ì ìƒì„±
        parking_spots = self.detect_parking_grid_from_corrected_image(corrected_image, vehicles)

        # 5. ì ìœ  ìƒíƒœ ë¶„ì„
        parking_spots = self.analyze_occupancy_with_id(vehicles, parking_spots)

        # 6. í†µê³„ ê³„ì‚°
        empty_count = sum(1 for spot in parking_spots if spot.status == ParkingSpotStatus.EMPTY)
        occupied_count = sum(1 for spot in parking_spots if spot.status == ParkingSpotStatus.OCCUPIED)

        stats = {
            'total_spots': len(parking_spots),
            'empty_spots': empty_count,
            'occupied_spots': occupied_count,
            'vehicles_detected': len(vehicles),
            'occupancy_rate': occupied_count / len(parking_spots) * 100 if parking_spots else 0,
            'perspective_corrected': transform_matrix is not None,
            'uniform_grid': True,
            'average_spot_area': np.mean([spot.area for spot in parking_spots]) if parking_spots else 0
        }

        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {stats}")

        return image, corrected_image, parking_spots, stats

    def draw_results_with_perspective(self, original_image: np.ndarray, corrected_image: np.ndarray,
                                    parking_spots: List[ParkingSpot], vehicles: List[Vehicle] = None) -> Tuple[np.ndarray, np.ndarray]:
        """ì›ë³¸ê³¼ ë³´ì •ëœ ì´ë¯¸ì§€ ëª¨ë‘ì— ê²°ê³¼ ì‹œê°í™”"""

        # ì›ë³¸ ì´ë¯¸ì§€ ê²°ê³¼
        original_result = original_image.copy()

        # ë³´ì •ëœ ì´ë¯¸ì§€ ê²°ê³¼
        corrected_result = corrected_image.copy()

        # ì£¼ì°¨ êµ¬ì—­ ê·¸ë¦¬ê¸° (ë³´ì •ëœ ì´ë¯¸ì§€ì—)
        for spot in parking_spots:
            x1, y1, x2, y2 = spot.bbox

            # ìƒíƒœë³„ ìƒ‰ìƒ
            if spot.status == ParkingSpotStatus.EMPTY:
                color = (0, 255, 0)  # ë…¹ìƒ‰
                text = "EMPTY"
            elif spot.status == ParkingSpotStatus.OCCUPIED:
                color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰
                text = "OCCUPIED"
            else:
                color = (0, 255, 255)  # ë…¸ë€ìƒ‰
                text = "UNKNOWN"

            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(corrected_result, (x1, y1), (x2, y2), color, 2)

            # IDì™€ ìƒíƒœ í…ìŠ¤íŠ¸
            cv2.putText(corrected_result, f"P{spot.id:02d}", (x1+5, y1+20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            cv2.putText(corrected_result, text, (x1+5, y1+40),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

            # ê²©ì ìœ„ì¹˜ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
            if spot.grid_position:
                row, col = spot.grid_position
                cv2.putText(corrected_result, f"({row},{col})", (x1+5, y1+55),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)

        # ì°¨ëŸ‰ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        if vehicles:
            for i, vehicle in enumerate(vehicles):
                x1, y1, x2, y2 = vehicle.bbox
                cv2.rectangle(corrected_result, (x1, y1), (x2, y2), (255, 0, 255), 3)
                cv2.putText(corrected_result, f"V{i+1} ({vehicle.confidence:.2f})",
                           (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)

                # ì°¨ëŸ‰ í¬ê¸° ì •ë³´
                w, h = vehicle.size
                cv2.putText(corrected_result, f"{w:.0f}x{h:.0f}",
                           (x1, y1-25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 255), 1)

        return original_result, corrected_result

    def save_results_with_perspective(self, original_image: np.ndarray, corrected_image: np.ndarray,
                                    parking_spots: List[ParkingSpot], stats: Dict, vehicles: List[Vehicle] = None,
                                    output_dir: str = "perspective_results"):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)

        # ê²°ê³¼ ì´ë¯¸ì§€
        original_result, corrected_result = self.draw_results_with_perspective(
            original_image, corrected_image, parking_spots, vehicles)

        cv2.imwrite(f"{output_dir}/perspective_original.jpg", original_result)
        cv2.imwrite(f"{output_dir}/perspective_corrected.jpg", corrected_result)

        # JSON ë°ì´í„°
        json_data = {
            'statistics': stats,
            'parking_spots': [
                {
                    'id': spot.id,
                    'bbox': [int(x) for x in spot.bbox],
                    'center': [int(x) for x in spot.center],
                    'area': float(spot.area),
                    'status': spot.status.value,
                    'confidence': float(spot.confidence),
                    'grid_position': spot.grid_position,
                    'corners': spot.corners
                }
                for spot in parking_spots
            ],
            'vehicles': [
                {
                    'bbox': [int(x) for x in vehicle.bbox],
                    'center': [int(x) for x in vehicle.center],
                    'size': [float(x) for x in vehicle.size],
                    'confidence': float(vehicle.confidence)
                }
                for vehicle in vehicles or []
            ]
        }

        with open(f"{output_dir}/perspective_analysis.json", 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_dir}/")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš— Perspective Transform ê¸°ë°˜ ì£¼ì°¨ì¥ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 50)

    # ëª¨ë¸ ê²½ë¡œ
    yolo_path = "../../yolov8n-obb.pt"

    # ê°ì§€ê¸° ì´ˆê¸°í™”
    detector = PerspectiveParkingDetector(yolo_path)

    # ì´ë¯¸ì§€ ì²˜ë¦¬
    image_path = "parkinglot1.jpg"

    if not os.path.exists(image_path):
        print(f"âŒ ì´ë¯¸ì§€ ì—†ìŒ: {image_path}")
        return

    try:
        # ë¶„ì„ ì‹¤í–‰
        original, corrected, spots, stats = detector.process_parking_lot_with_perspective(image_path)

        # ì°¨ëŸ‰ ì •ë³´ë„ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
        vehicles = detector.detect_vehicles_with_obb(corrected)

        # ê²°ê³¼ ì €ì¥
        detector.save_results_with_perspective(original, corrected, spots, stats, vehicles)

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 50)
        print("ğŸ“Š ìµœì¢… ê²°ê³¼")
        print("=" * 50)
        print(f"ğŸ…¿ï¸  ì´ ì£¼ì°¨êµ¬ì—­: {stats['total_spots']}ê°œ")
        print(f"ğŸŸ¢ ë¹ˆ ìë¦¬: {stats['empty_spots']}ê°œ")
        print(f"ğŸ”´ ì ìœ ëœ ìë¦¬: {stats['occupied_spots']}ê°œ")
        print(f"ğŸš— ê°ì§€ëœ ì°¨ëŸ‰: {stats['vehicles_detected']}ëŒ€")
        print(f"ğŸ“ˆ ì ìœ ìœ¨: {stats['occupancy_rate']:.1f}%")
        print(f"ğŸ”§ Perspective ë³´ì •: {'âœ…' if stats['perspective_corrected'] else 'âŒ'}")
        print(f"ğŸ“ ê· ì¼ ê²©ì: {'âœ…' if stats['uniform_grid'] else 'âŒ'}")
        print(f"ğŸ“ í‰ê·  êµ¬ì—­ í¬ê¸°: {stats['average_spot_area']:.0f} í”½ì…€Â²")

        # ì‹œê°í™”
        plt.figure(figsize=(20, 12))

        plt.subplot(2, 3, 1)
        plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))
        plt.title("ì›ë³¸ ì´ë¯¸ì§€")
        plt.axis('off')

        plt.subplot(2, 3, 2)
        plt.imshow(cv2.cvtColor(corrected, cv2.COLOR_BGR2RGB))
        plt.title("Perspective ë³´ì •ëœ ì´ë¯¸ì§€")
        plt.axis('off')

        # ê²°ê³¼ ì´ë¯¸ì§€ë“¤
        original_result, corrected_result = detector.draw_results_with_perspective(original, corrected, spots, vehicles)

        plt.subplot(2, 3, 3)
        plt.imshow(cv2.cvtColor(corrected_result, cv2.COLOR_BGR2RGB))
        plt.title(f"ë¶„ì„ ê²°ê³¼ ({stats['total_spots']}ê°œ êµ¬ì—­)")
        plt.axis('off')

        plt.subplot(2, 3, 4)
        labels = ['ë¹ˆ ìë¦¬', 'ì ìœ ']
        sizes = [stats['empty_spots'], stats['occupied_spots']]
        colors = ['lightgreen', 'lightcoral']
        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
        plt.title("ì£¼ì°¨ í˜„í™©")

        plt.subplot(2, 3, 5)
        categories = ['ì´ êµ¬ì—­', 'ë¹ˆ ìë¦¬', 'ì ìœ ', 'ì°¨ëŸ‰']
        values = [stats['total_spots'], stats['empty_spots'],
                 stats['occupied_spots'], stats['vehicles_detected']]
        bars = plt.bar(categories, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
        plt.title("í†µê³„ ìš”ì•½")
        plt.ylabel("ê°œìˆ˜")

        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar, value in zip(bars, values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    str(value), ha='center', va='bottom')

        plt.subplot(2, 3, 6)
        # ì£¼ì°¨ êµ¬ì—­ë³„ ìƒíƒœ íˆíŠ¸ë§µ
        if spots:
            max_row = max(spot.grid_position[0] for spot in spots if spot.grid_position) + 1
            max_col = max(spot.grid_position[1] for spot in spots if spot.grid_position) + 1

            heatmap = np.zeros((max_row, max_col))
            for spot in spots:
                if spot.grid_position:
                    row, col = spot.grid_position
                    if spot.status == ParkingSpotStatus.OCCUPIED:
                        heatmap[row, col] = 1
                    elif spot.status == ParkingSpotStatus.EMPTY:
                        heatmap[row, col] = 0.5

            plt.imshow(heatmap, cmap='RdYlGn_r', aspect='auto')
            plt.title("ì£¼ì°¨ êµ¬ì—­ íˆíŠ¸ë§µ")
            plt.xlabel("ì—´")
            plt.ylabel("í–‰")
            plt.colorbar(label="ì ìœ  ìƒíƒœ")

        plt.tight_layout()
        plt.savefig("perspective_parking_analysis.png", dpi=150, bbox_inches='tight')
        plt.show()

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()